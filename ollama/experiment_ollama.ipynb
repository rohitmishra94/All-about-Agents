{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6f6ca8-0fd0-470e-95e6-8a44254d0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/swarm.git --quiet\n",
    "!pip install ollama openai --quiet\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e158ae6c-9b0a-43d0-940e-31d65cf6a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install tiktoken==0.8.0 openai==1.59.5 chromadb==0.6.2 FlagEmbedding==1.3.3 pymupdf4llm==0.0.17 tenacity==9.0.0 ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7095eb5d-0078-4dd9-84b6-c4191a3ea75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004d14ff-7335-4cbe-9bba-f739bd68a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ollama-pydantic/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from FlagEmbedding import FlagReranker\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import pymupdf4llm\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac6adbd-2291-48d8-a8b7-bf60b772ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 165347.07it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def embedding_function_bge(text_list):\n",
    "    return emb_model.encode(text_list, return_dense=True)['dense_vecs']\n",
    "\n",
    "\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embeddings = embedding_function_bge(input)\n",
    "        return embeddings\n",
    "\n",
    "emb_model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)\n",
    "default_ef = MyEmbeddingFunction()\n",
    "client = chromadb.PersistentClient(path=\"chromadb_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14cefa6-1303-462b-acd1-d0b64acf9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04208 , -0.001314, -0.05435 , ...,  0.007812, -0.02707 ,\n",
       "         0.0521  ]], shape=(1, 1024), dtype=float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_function_bge(['hi there'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d8cd14-4ab7-4cf4-873f-164a1ee89eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import AsyncClient\n",
    "# ollama_client = AsyncClient()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3113fe21-83fa-4fc0-9306-99b11854d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_BASE_URL']='http://localhost:11434/v1' \n",
    "# os.environ['SWARM_DEFAULT_MODEL']='llama3-groq-tool-use'\n",
    "os.environ['OPENAI_API_KEY']='ollama'\n",
    "\n",
    "\n",
    "# model = 'llama3-groq-tool-use'\n",
    "# model = 'llama3.2:3b-instruct-fp16'\n",
    "# model = 'llama3.1:8b'\n",
    "# model = 'qwen2.5:14b'\n",
    "model = 'qwen2.5:14b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8336426-43a7-4b1f-ac5a-9de0c86e9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_texts(texts, chunk_size=100, overlap=30):\n",
    "    \"\"\"Process a list of texts, splitting them into chunks of specified size with overlap,\n",
    "    and accumulating shorter texts.\"\"\"\n",
    "    accumulated_words = []  # Accumulate words from texts shorter than chunk_size\n",
    "    final_chunks = []  # Store the final chunks of text\n",
    "\n",
    "    for text in texts.split():\n",
    "        accumulated_words.append(text)\n",
    "\n",
    "        while len(accumulated_words) >= chunk_size:\n",
    "            # Take the first chunk_size words for the current chunk\n",
    "            chunk = \" \".join(accumulated_words[:chunk_size])\n",
    "            final_chunks.append(chunk)\n",
    "            # Remove words from the start of the accumulated_words, considering overlap\n",
    "            accumulated_words = accumulated_words[chunk_size - overlap:]\n",
    "\n",
    "    # If there are any remaining words, form the last chunk\n",
    "    if accumulated_words:\n",
    "        final_chunks.append(\" \".join(accumulated_words))\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def get_unique_text_indices(text_list):\n",
    "    unique_texts = {}\n",
    "    unique_indices = []\n",
    "\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text not in unique_texts:\n",
    "            unique_texts[text] = i\n",
    "            unique_indices.append(i)\n",
    "\n",
    "    return unique_indices\n",
    "\n",
    "def create_pdf_collection(pdf_path):\n",
    "    \"\"\"\n",
    "    Process a PDF file and add its chunks to a collection.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        collection: The collection object to add documents to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Create collection Running....')\n",
    "        md_text = pymupdf4llm.to_markdown(pdf_path,show_progress=False)\n",
    "        all_chunks = process_texts(md_text, chunk_size=500, overlap=50)\n",
    "        collection = client.get_or_create_collection(name='database',embedding_function=default_ef)\n",
    "        logger.info(collection)\n",
    "\n",
    "        for idx, chunk in tqdm(enumerate(all_chunks)):\n",
    "            id_ = str(idx)\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                ids=[id_]\n",
    "            )\n",
    "        status = 'success'\n",
    "        print('Done.')\n",
    "        return status\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating pdf collection: {str(e)}\", exc_info=True)\n",
    "        return f'Sorry for inconvenience. Error creating pdf collection. Please contact support.'\n",
    "\n",
    "\n",
    "def get_full_context(query, collection, n_results=5, top=2):\n",
    "\n",
    "    ''''\n",
    "    Get the context from database for given query.\n",
    "\n",
    "    Args:\n",
    "        query: Query string\n",
    "        collection_name: Database collection object\n",
    "        n_results: Number of results to retrieve\n",
    "        top: Number of top results to return\n",
    "    Returns:\n",
    "          str: context based on query.\n",
    "    '''\n",
    "\n",
    "    logger.info(f'quering collection---> {collection}')\n",
    "\n",
    "    result = collection.query(query_texts = query,n_results=n_results)\n",
    "    texts = result['documents'][0]\n",
    "    ids = result['ids'][0]\n",
    "    unique_indices = get_unique_text_indices(texts)\n",
    "    unique_docs = [texts[x] for x in unique_indices]\n",
    "    unique_ids = [ids[x] for x in unique_indices]\n",
    "    ## colbert\n",
    "    query_col = emb_model.encode([query],return_colbert_vecs=True)\n",
    "    docs_col = emb_model.encode(unique_docs,return_colbert_vecs=True)\n",
    "    colber_scores = []\n",
    "    for vectors in docs_col['colbert_vecs']:\n",
    "        colber_scores.append(emb_model.colbert_score(query_col['colbert_vecs'][0],vectors).numpy())\n",
    "\n",
    "    ## full_context_colbert\n",
    "    full_context_scores = []\n",
    "    full_context_ids = []\n",
    "    for id in unique_ids:\n",
    "        pre_id,post_id = str(int(id)-1), str(int(id)+1)\n",
    "        # print(pre_id,id,post_id)\n",
    "        full_context_ids.append([pre_id,id,post_id])\n",
    "        full_context=collection.get(ids=[f'{pre_id}',f'{id}',f'{post_id}'])['documents']\n",
    "        full_context = ''.join(full_context)\n",
    "        full_context_colber_vec = emb_model.encode([full_context],return_colbert_vecs=True)\n",
    "        full_context_colber_score = emb_model.colbert_score(query_col['colbert_vecs'][0],full_context_colber_vec['colbert_vecs'][0]).numpy()\n",
    "\n",
    "        full_context_scores.append(full_context_colber_score)\n",
    "\n",
    "    all_scores = [2*full_context_scores[i]+0.9*colber_scores[i] for i in range(len(colber_scores))]\n",
    "    sorted_indices = [index for index, _ in sorted(enumerate(all_scores), key=lambda x: x[1], reverse=True)]\n",
    "    top_context_ids_list = [full_context_ids[index] for index in sorted_indices][:top]\n",
    "    flattened_list = np.array(top_context_ids_list).flatten().tolist()\n",
    "    top_ids = list(set(flattened_list))\n",
    "    top_ids.sort()\n",
    "    top_context = collection.get(ids=top_ids)['documents']\n",
    "    # top_context = ''.join(top_context)\n",
    "\n",
    "    logger.info(f'context retrieved from collection---> {collection}')\n",
    "    return top_context,top_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db53aa6d-8837-4f04-b897-2e8804ff090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_pdf_collection('/workspace/ollama-pydantic/llama3-report.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf44538d-ca28-43e3-bf3b-fa75627d0058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83fb528-234f-42ca-8572-150dcf096c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# client = Client()\n",
    "# response = client.create(\n",
    "#   model='my-assistant',\n",
    "#   from_='llama3.2',\n",
    "#   system='You are mario from Super Mario Bros.',\n",
    "#   stream=False,\n",
    "# )\n",
    "# print(response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4051b9a3-3eba-4800-952a-72f1fb4d5908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context found\n"
     ]
    }
   ],
   "source": [
    "def get_context_based_on_collection(query):\n",
    "  '''\n",
    "  Input: user query\n",
    "  Return the query answer based on context available in database.\n",
    "  '''\n",
    "  collection = client.get_or_create_collection(name='database',embedding_function=default_ef)\n",
    "\n",
    "  context, context_idx = get_full_context(query, collection, n_results=5, top=2)\n",
    "\n",
    "  if context:\n",
    "    print('context found')\n",
    "\n",
    "    return ' '.join(context)\n",
    "  else:\n",
    "    return f'No context found for given {query} through collection'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2c5c6c-2ecc-4710-8359-0daba8385f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context\n",
    "\n",
    "query = 'trainign methods involved'\n",
    "context = get_context_based_on_collection(query)\n",
    "def get_answer(context,query):\n",
    "    # from ollama import AsyncClient\n",
    "    from ollama import Client\n",
    "    messages = [\n",
    "        {\n",
    "          'content': f'Based on given context: {context}, answer this query: {query} in full detail. always answer in full detailed and pointers. if context is irrelevant, say no relevant context found to answer query.',\n",
    "          'role': 'user',\n",
    "        },\n",
    "      ]\n",
    "    \n",
    "    ollama_client = Client()\n",
    "    response = ollama_client.chat(model, messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8084ce22-5b46-4fee-aca5-172d5ecb5317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The training process for developing Llama 3 involves several comprehensive stages designed to enhance the model's performance, alignment with human preferences, and robustness across various tasks. Below is a detailed breakdown of each stage:\\n\\n### 1. **Reward Model Training**\\n   - **Objective:** To create a reward model that can evaluate generated responses based on human preferences.\\n   - **Data Preparation:**\\n     - Preference data consists of triplets containing edited response, chosen response, and rejected response in clear ranking order (edited > chosen > rejected).\\n     - Triplets are constructed to ensure the reward model learns from human judgments effectively.\\n   - **Training Process:**\\n     - The training involves concatenating prompts with randomly shuffled responses into single rows for a more comprehensive understanding of the context.\\n     - This step trains a neural network that can predict which response is preferred based on its alignment with human preferences.\\n\\n### 2. **Rejection Sampling**\\n   - **Objective:** To filter out poor quality data, ensuring only high-quality data proceeds to subsequent fine-tuning stages.\\n   - **Process:**\\n     - The trained reward model evaluates responses generated by the language model in response to human annotation prompts.\\n     - Poorly rated responses are discarded through rejection sampling, retaining only those that meet a certain threshold of quality based on the reward model’s evaluation.\\n\\n### 3. **Supervised Fine-Tuning (SFT)**\\n   - **Objective:** To fine-tune the pre-trained language model using cross entropy loss and synthetic data to enhance performance.\\n   - **Training Process:**\\n     - The model is trained by minimizing the cross entropy loss between predicted target tokens and actual labels while masking losses on prompt tokens.\\n     - This stage includes training with a variety of curated datasets that cover diverse scenarios, ensuring comprehensive knowledge acquisition.\\n     - Training parameters include 8.5K to 9K steps and a learning rate set at \\\\(10^{-5}\\\\).\\n\\n### 4. **Direct Preference Optimization (DPO)**\\n   - **Objective:** To optimize the model further by refining its alignment with human preferences using preference data from previous rounds.\\n   - **Training Process:**\\n     - DPO employs negative log-likelihood (NLL) loss on chosen sequences to stabilize training and maintain desired formatting.\\n     - Special modifications include masking out special formatting tokens, which helps in maintaining consistency across responses.\\n\\n### 5. **Model Averaging**\\n   - **Objective:** To enhance the model’s robustness by averaging models trained with different datasets or hyperparameters at each stage (reward model, SFT, DPO).\\n   - **Process:**\\n     - The final model is created through ensemble techniques that average out multiple configurations.\\n     - This technique mitigates overfitting and ensures broader coverage of possible input scenarios, enhancing the model’s generalization capabilities.\\n\\n### Summary\\nEach training method in Llama 3 development is meticulously designed to address specific challenges. These include:\\n- **Reward Model Training:** Establishing a baseline for human preference evaluation.\\n- **Rejection Sampling:** Ensuring high-quality data through filtering.\\n- **SFT:** Fine-tuning with cross entropy loss and diverse datasets.\\n- **DPO:** Optimizing alignment with human preferences using NLL loss.\\n- **Model Averaging:** Enhancing robustness by averaging across multiple training configurations.\\n\\nThis comprehensive approach ensures that Llama 3 achieves high performance, maintains robustness, and aligns well with human preferences across a variety of benchmarks and tasks.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(context,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def31cd9-814a-4f6d-92ee-876459560812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd91bea3-a485-4f81-88fb-1b774d35a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from swarm import Swarm, Agent\n",
    "\n",
    "\n",
    "# Swarm_client = Swarm()\n",
    "\n",
    "# english_agent = Agent(\n",
    "#     name=\"English Agent\",\n",
    "#     instructions=\"You only speak English.\",\n",
    "#     model = model\n",
    "# )\n",
    "\n",
    "# spanish_agent = Agent(\n",
    "#     name=\"Spanish Agent\",\n",
    "#     instructions=\"You only speak Spanish.\",\n",
    "#     model = model\n",
    "# )\n",
    "\n",
    "\n",
    "# def transfer_to_spanish_agent():\n",
    "#     \"\"\"Transfer spanish speaking users immediately.\"\"\"\n",
    "#     return spanish_agent\n",
    "\n",
    "\n",
    "# english_agent.functions.append(transfer_to_spanish_agent)\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Hola. ¿Como estás?\"}]\n",
    "# response = client.run(agent=english_agent, messages=messages)\n",
    "\n",
    "# print(response.messages[-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd9b1ca-9024-4441-9cef-9881d5c21c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o-mini\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "\n",
    "class Response(BaseModel):\n",
    "    agent: Optional[Agent]\n",
    "    messages: list\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)\n",
    "\n",
    "\n",
    "from ollama import AsyncClient\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#       'content': f'Based on given context: {context}, answer this query: {query} in full detail. always answer in full detailed and pointers. if context is irrelevant, say no relevant context found to answer query.',\n",
    "#       'role': 'user',\n",
    "#     },\n",
    "#   ]\n",
    "\n",
    "\n",
    "# response = await ollama_client.chat(model, messages=messages)\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f9bb8892-26c4-4561-90cc-32e35253d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_to_answer_agent(**args):\n",
    "  '''\n",
    "  Transfer session to answering agent.\n",
    "\n",
    "  '''\n",
    "  return answer_agent\n",
    "\n",
    "\n",
    "def transfer_to_main_agent(**args):\n",
    "  '''\n",
    "  Transfer session to main agent.\n",
    "\n",
    "  '''\n",
    "  return main_agent\n",
    "\n",
    "main_agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Main Agent\",\n",
    "    instructions='''\n",
    "You are the Main Agent.\n",
    "\n",
    "### DUTIES:\n",
    "1. **User Interaction & Greeting**:\n",
    "   - Always greet the user during the first interaction.\n",
    "   - Clearly state your purpose as the Main Agent.\n",
    "  \n",
    "2. **Providing Responses**:\n",
    "   - Ask user for query regarding internal database only.\n",
    "   - Transfer session to answering agent 'transfer_to_answer_agent'\n",
    "   - Summarize the findings of the Answering Agent concisely and accurately.\n",
    "   - Do not provide additional interpretations or assumptions beyond what the Answering Agent provides.\n",
    "   - \n",
    "\n",
    "3. **Role Adherence**:\n",
    "   - Do not assume any role other than that of a helpful assistant.\n",
    "   - Do not generate responses independently; rely solely on the Answering Agent’s findings.\n",
    "\n",
    "''',\n",
    "    tools=[transfer_to_answer_agent],\n",
    "\n",
    ")\n",
    "\n",
    "answer_agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Answering Agent\",\n",
    "    instructions='''\n",
    "You are the Answering Agent, responsible for retrieving information from the internal database and responding to user queries.\n",
    "\n",
    "### DUTIES:\n",
    "1. **Query Handling**:\n",
    "   - Answer user queries using the available database.\n",
    "   - Retrieve relevant context using the provided function `get_context_based_on_collection`.\n",
    "\n",
    "2. **Function Usage**:\n",
    "   - Use `get_context_based_on_collection` to gather context around the user query before responding.\n",
    "\n",
    "3. **Role Adherence**:\n",
    "   - Ensure all responses are based solely on the available database—do not generate assumptions or external knowledge.\n",
    "   - if user end with ok, thanks , and other experession to conclude, then conclude the session\n",
    "\n",
    "\n",
    "''',\n",
    "    tools=[get_context_based_on_collection,transfer_to_main_agent],\n",
    "    # tool_choice='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "afce1be4-deaa-4de2-877e-913032561389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"sample_function\",\n",
      "    \"description\": \"This is my docstring. Call this function when you want.\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"param_1\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"param_2\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"the_third_one\": {\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"some_optional\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"param_1\",\n",
      "        \"param_2\",\n",
      "        \"the_third_one\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import json\n",
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "def sample_function(param_1, param_2, the_third_one: int, some_optional=\"John Doe\"):\n",
    "    \"\"\"\n",
    "    This is my docstring. Call this function when you want.\n",
    "    \"\"\"\n",
    "    print(\"Hello, world\")\n",
    "\n",
    "schema =  function_to_schema(sample_function)\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b7a48729-726a-48cf-8524-b9127177803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_client = AsyncClient()\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#       'content': f'transfer to main agent',\n",
    "#       'role': 'user',\n",
    "#     },\n",
    "#   ]\n",
    "\n",
    "\n",
    "# response = await ollama_client.chat(model, messages=messages,tools = [transfer_to_main_agent])\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "95bed3d2-7ebe-4a89-9880-52b774af49b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('52ab408c-ea9a-4783-9960-3c0a5f080560')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "67b9f5de-7aad-4e67-a325-5d5f97fd286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_full_turn(agent, messages):\n",
    "\n",
    "    current_agent = agent\n",
    "    num_init_messages = len(messages)\n",
    "    messages = messages.copy()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # turn python functions into tools and save a reverse map\n",
    "        tool_schemas = [function_to_schema(tool) for tool in current_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "        # === 1. get openai completion ===\n",
    "        response = await ollama_client.chat(agent.model,  messages=[{\"role\": \"system\", \"content\": current_agent.instructions}]\n",
    "            + messages,tools=tool_schemas)\n",
    "        message = response['message']\n",
    "        messages.append(response.message)\n",
    "\n",
    "        if message.content:  # print agent response\n",
    "            print(f\"{current_agent.name}:\", message.content)\n",
    "\n",
    "        if not message.tool_calls:  # if finished handling tool calls, break\n",
    "            break\n",
    "\n",
    "        # === 2. handle tool calls ===\n",
    "\n",
    "        for tool_call in message.tool_calls:\n",
    "            result = execute_tool_call(tool_call, tools, current_agent.name)\n",
    "\n",
    "            if type(result) is Agent:  # if agent transfer, update current agent\n",
    "                current_agent = result\n",
    "                result = (\n",
    "                    f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "                )\n",
    "\n",
    "            result_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": uuid.uuid4(),\n",
    "                \"content\": result,\n",
    "                'name': tool_call.function.name\n",
    "            }\n",
    "            messages.append(result_message)\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    return Response(agent=current_agent, messages=messages[num_init_messages:])\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = tool_call.function.arguments\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "53372884-c9b6-4463-b5d0-85ecfed74ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: transfer_to_answer_agent({'args': ''})\n",
      "Answering Agent: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    }
   ],
   "source": [
    "agent = main_agent\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    user = input(\"User: \").strip()\n",
    "    if user.lower()=='q':\n",
    "      break\n",
    "    messages.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "    response = await run_full_turn(agent, messages)\n",
    "    agent = response.agent\n",
    "    messages.extend(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bac5ad52-cc07-462b-82c2-ae390d78a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = main_agent\n",
    "# messages = []\n",
    "\n",
    "# while True:\n",
    "#     user_msg = input(\"User: \").strip()\n",
    "#     if user_msg.lower()=='q':\n",
    "#       break\n",
    "#     user_template = [{\"role\": \"user\", \"content\": f\"{user_msg}\"}]\n",
    "#     messages.extend(user_template)\n",
    "#     response = Swarm_client.run(agent=agent,messages = messages)\n",
    "#     response_content = response.messages[-1][\"content\"]\n",
    "#     print(f'{response.agent.name}: {response_content}')\n",
    "#     agent = response.agent\n",
    "#     messages.extend(response.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9710adb2-fc12-438f-b2bb-8e00a5c4ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6c8be-f4b4-4441-bd77-f5a268eddf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"  # Replace with the correct model ID\n",
    "save_path = \"./models/bge-m3\"\n",
    "\n",
    "# Download and save the model locally\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "print(\"Model downloaded and saved locally at:\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

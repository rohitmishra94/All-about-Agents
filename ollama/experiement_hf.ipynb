{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf8S_Hz53H0W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SgL7SZHjkTK",
    "outputId": "45fb07f5-871f-4eb5-a72b-e46215834278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install flash_attn==2.5.8\n",
    "!pip install torch accelerate transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-l85-JpjhSso",
    "outputId": "1fba0077-7ac7-4784-888c-a264a1a7e973"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken==0.8.0 chromadb==0.6.2 FlagEmbedding==1.3.3 pymupdf4llm==0.0.17 tenacity==9.0.0 python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uR8eQkOW-xcY"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "56efd864c9974278a2e450c9bf5d792d",
      "be1f5ba9686544c69458093252c69d65",
      "f4b191c2dac1481a927056ca922249ac",
      "b331c102a6e843be95d590c5d19e13eb",
      "e105acc2a17b4036ab64df8112504ee8",
      "c020475eb09646888ef3cc0a4a33a3ba",
      "77faf11a8f0e4a68bb5cc9d8df10bd91",
      "b1e69143ec654199b3703f4174104870",
      "c12fc4072d5d4485b77b0466dfe9ab9d",
      "f11a53238f8b4bf9a60f2595c3229d2c",
      "1f72321af9f944b5808ee8665c539d83"
     ]
    },
    "id": "vlDm67HG-yl7",
    "outputId": "923e99d0-ffe9-41af-f3dd-f353a5fa669e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845412783d46436d82886c819d448b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zn2luVDI7ZUB"
   },
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from FlagEmbedding import FlagReranker\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import pymupdf4llm\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d812b898cffd466ebc086b50cfdf3f3f",
      "ec8f97931fb241cd9463bfb137f0af9d",
      "a8d5639fc4cb4e579403115cbe11a884",
      "214d14ca123f4a3a8484497d3a5e505b",
      "cfcef53b8249443c8f77612a925f61a0",
      "54e51adc084f4cc4aaf7847ebc115a6d",
      "62db395b10c04631ba5eedfe6ee6bc01",
      "3f796b13452d4c18bff69a4e4331a331",
      "8d9876d79fdb4c829356bdc91e133681",
      "ff866aaf4527499dad53cd944d510a69",
      "084516d8a47c4cf1b7dbf4ce2cc4fc75"
     ]
    },
    "id": "WcNksEL67ZRB",
    "outputId": "0a6a7fe0-5c57-40d7-c453-1d20b5c672f1"
   },
   "outputs": [],
   "source": [
    "def embedding_function_bge(text_list):\n",
    "    return emb_model.encode(text_list, return_dense=True)['dense_vecs']\n",
    "\n",
    "\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embeddings = embedding_function_bge(input)\n",
    "        return embeddings\n",
    "\n",
    "emb_model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True)\n",
    "default_ef = MyEmbeddingFunction()\n",
    "client = chromadb.PersistentClient(path=\"chromadb_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "_-0YnC6-7ZN_"
   },
   "outputs": [],
   "source": [
    "def process_texts(texts, chunk_size=100, overlap=30):\n",
    "    \"\"\"Process a list of texts, splitting them into chunks of specified size with overlap,\n",
    "    and accumulating shorter texts.\"\"\"\n",
    "    accumulated_words = []  # Accumulate words from texts shorter than chunk_size\n",
    "    final_chunks = []  # Store the final chunks of text\n",
    "\n",
    "    for text in texts.split():\n",
    "        accumulated_words.append(text)\n",
    "\n",
    "        while len(accumulated_words) >= chunk_size:\n",
    "            # Take the first chunk_size words for the current chunk\n",
    "            chunk = \" \".join(accumulated_words[:chunk_size])\n",
    "            final_chunks.append(chunk)\n",
    "            # Remove words from the start of the accumulated_words, considering overlap\n",
    "            accumulated_words = accumulated_words[chunk_size - overlap:]\n",
    "\n",
    "    # If there are any remaining words, form the last chunk\n",
    "    if accumulated_words:\n",
    "        final_chunks.append(\" \".join(accumulated_words))\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def get_unique_text_indices(text_list):\n",
    "    unique_texts = {}\n",
    "    unique_indices = []\n",
    "\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text not in unique_texts:\n",
    "            unique_texts[text] = i\n",
    "            unique_indices.append(i)\n",
    "\n",
    "    return unique_indices\n",
    "\n",
    "def create_pdf_collection(pdf_path):\n",
    "    \"\"\"\n",
    "    Process a PDF file and add its chunks to a collection.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        collection: The collection object to add documents to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md_text = pymupdf4llm.to_markdown(pdf_path,show_progress=True)\n",
    "        all_chunks = process_texts(md_text, chunk_size=500, overlap=50)\n",
    "        collection = client.get_or_create_collection(name='database',embedding_function=default_ef)\n",
    "        logger.info(collection)\n",
    "\n",
    "        for idx, chunk in tqdm(enumerate(all_chunks)):\n",
    "            id_ = str(idx)\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                ids=[id_]\n",
    "            )\n",
    "        status = 'success'\n",
    "        return status,collection_name\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating pdf collection: {str(e)}\", exc_info=True)\n",
    "        return f'Sorry for inconvenience. Error creating pdf collection. Please contact support.'\n",
    "\n",
    "\n",
    "def get_full_context(query, collection, n_results=5, top=2):\n",
    "\n",
    "    ''''\n",
    "    Get the context from database for given query.\n",
    "\n",
    "    Args:\n",
    "        query: Query string\n",
    "        collection_name: Database collection object\n",
    "        n_results: Number of results to retrieve\n",
    "        top: Number of top results to return\n",
    "    Returns:\n",
    "          str: context based on query.\n",
    "    '''\n",
    "\n",
    "    logger.info(f'quering collection---> {collection}')\n",
    "\n",
    "    result = collection.query(query_texts = query,n_results=n_results)\n",
    "    texts = result['documents'][0]\n",
    "    ids = result['ids'][0]\n",
    "    unique_indices = get_unique_text_indices(texts)\n",
    "    unique_docs = [texts[x] for x in unique_indices]\n",
    "    unique_ids = [ids[x] for x in unique_indices]\n",
    "    ## colbert\n",
    "    query_col = emb_model.encode([query],return_colbert_vecs=True)\n",
    "    docs_col = emb_model.encode(unique_docs,return_colbert_vecs=True)\n",
    "    colber_scores = []\n",
    "    for vectors in docs_col['colbert_vecs']:\n",
    "        colber_scores.append(emb_model.colbert_score(query_col['colbert_vecs'][0],vectors).numpy())\n",
    "\n",
    "    ## full_context_colbert\n",
    "    full_context_scores = []\n",
    "    full_context_ids = []\n",
    "    for id in unique_ids:\n",
    "        pre_id,post_id = str(int(id)-1), str(int(id)+1)\n",
    "        # print(pre_id,id,post_id)\n",
    "        full_context_ids.append([pre_id,id,post_id])\n",
    "        full_context=collection.get(ids=[f'{pre_id}',f'{id}',f'{post_id}'])['documents']\n",
    "        full_context = ''.join(full_context)\n",
    "        full_context_colber_vec = emb_model.encode([full_context],return_colbert_vecs=True)\n",
    "        full_context_colber_score = emb_model.colbert_score(query_col['colbert_vecs'][0],full_context_colber_vec['colbert_vecs'][0]).numpy()\n",
    "\n",
    "        full_context_scores.append(full_context_colber_score)\n",
    "\n",
    "    all_scores = [2*full_context_scores[i]+0.9*colber_scores[i] for i in range(len(colber_scores))]\n",
    "    sorted_indices = [index for index, _ in sorted(enumerate(all_scores), key=lambda x: x[1], reverse=True)]\n",
    "    top_context_ids_list = [full_context_ids[index] for index in sorted_indices][:top]\n",
    "    flattened_list = np.array(top_context_ids_list).flatten().tolist()\n",
    "    top_ids = list(set(flattened_list))\n",
    "    top_ids.sort()\n",
    "    top_context = collection.get(ids=top_ids)['documents']\n",
    "    # top_context = ''.join(top_context)\n",
    "\n",
    "    logger.info(f'context retrieved from collection---> {collection}')\n",
    "    return top_context,top_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VK65QFylEi-i",
    "outputId": "e9d4666e-0acb-49c7-ed8a-b679aaa4b5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6, 8]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = [3,6,2,8]\n",
    "ll.sort()\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZNuFGOt-gpx",
    "outputId": "95733597-173a-4e7e-a61c-b3fbc6bfdbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /workspace/llama3-report.pdf...\n",
      "[                                        ] (0/92[                                        ] ( 1/9[                                        ] ( 2/92[=                                       ] ( 3/[=                                       ] ( 4/9[==                                      ] ( 5/9[==                                      ] ( 6/92[===                                     ] ( 7/[===                                     ] ( 8/[===                                     ] ( 9/9[====                                    ] (10/9[====                                    ] (11/92[=====                                   ] (12/[=====                                   ] (13/9[======                                  ] (14/9[======                                  ] (15/9[======                                  ] (16/92[=======                                 ] (17/[=======                                 ] (18/9[========                                ] (19/9[========                                ] (20/92[=========                               ] (21/92[=========                               ] (22/[==========                              ] (23/92[==========                              ] (24/9[==========                              ] (25/92[===========                             ] (26/[===========                             ] (27/9[============                            ] (28/9[============                            ] (29/92[=============                           ] (30/[=============                           ] (31/[=============                           ] (32/9[==============                          ] (33/9[==============                          ] (34/92[===============                         ] (35/[===============                         ] (36/9[================                        ] (37/9[================                        ] (38/9[================                        ] (39/92[=================                       ] (40/92[=================                       ] (41/9[==================                      ] (42/9[==================                      ] (43/92[===================                     ] (44/[===================                     ] (45/[====================                    ] (46/92[====================                    ] (47/9[====================                    ] (48/92[=====================                   ] (49/[=====================                   ] (50/9[======================                  ] (51/9[======================                  ] (52/92[=======================                 ] (53/[=======================                 ] (54/92[=======================                 ] (55/9[========================                ] (56/9[========================                ] (57/92[=========================               ] (58/92[=========================               ] (59/9[==========================              ] (60/9[==========================              ] (61/9[==========================              ] (62/92[===========================             ] (63/[===========================             ] (64/9[============================            ] (65/9[============================            ] (66/92[=============================           ] (67/[=============================           ] (68/[==============================          ] (69/92[==============================          ] (70/9[==============================          ] (71/92[===============================         ] (72/[===============================         ] (73/9[================================        ] (74/9[================================        ] (75/92[=================================       ] (76/[=================================       ] (77/[=================================       ] (78/9[==================================      ] (79/9[==================================      ] (80/92[===================================     ] (81/[===================================     ] (82/9[====================================    ] (83/9[====================================    ] (84/9[====================================    ] (85/92[=====================================   ] (86/[=====================================   ] (87/9[======================================  ] (88/9[======================================  ] (89/92[======================================= ] (90/[======================================= ] (91/[========================================] (92/92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [00:13,  8.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('success', 'llama3-report.pdf')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pdf_collection('/workspace/llama3-report.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x8I_e8LY-gfD"
   },
   "outputs": [],
   "source": [
    "# collection_name = 'llama3-report.pdf'\n",
    "# collection = client.get_or_create_collection(name=collection_name,embedding_function=default_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgAdxjDC_4fZ",
    "outputId": "eddc3247-e90c-452e-e8ba-bcabd5dee476"
   },
   "outputs": [],
   "source": [
    "# query = 'what is training algorithms used here'\n",
    "# get_full_context(query, collection, n_results=5, top=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "W-854K_YSqnD"
   },
   "outputs": [],
   "source": [
    "def generate(prompt, model, tokenizer, add_special_tokens=True):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=add_special_tokens,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True)\n",
    "\n",
    "    result = outputs[0][inputs[\"input_ids\"].shape[-1]:-1]\n",
    "    return tokenizer.decode(result, skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kqb7Cbn9bW5c",
    "outputId": "eca09b31-621b-446f-c6c7-7e3b4dc1aa9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "        'hi there',\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "    ).to(model.device)\n",
    "inputs[\"input_ids\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "ZX5ocjTSSqh7"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"dict\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_definitions = \"\"\"[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "febYrMDlSqec",
    "outputId": "82a1c278-6951-4a8a-9d60-8beac0dfcbb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "collection\n",
      "n_results=5\n",
      "top=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'get_full_context',\n",
       " 'description': \"'\\n    Get the context from database for given query.\\n\\n    Args:\\n        query: Query string\\n        collection_name: Database collection object\\n        n_results: Number of results to retrieve\\n        top: Number of top results to return\\n    Returns:\\n          str: context based on query.\",\n",
       " 'parameters': {'type': 'dict',\n",
       "  'properties': {'query': {'type': 'string'},\n",
       "   'collection': {'type': 'string'},\n",
       "   'n_results': {'type': 'string'},\n",
       "   'top': {'type': 'string'}},\n",
       "  'required': ['query', 'collection']}}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_to_schema(get_full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "ginLBF6EXdAb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# def execute_tool_call(tool_call, tools, agent_name):\n",
    "#     name = tool_call.function.name\n",
    "#     args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "#     print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "#     return tools[name](**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "b-19tDr-Xdqs"
   },
   "outputs": [],
   "source": [
    "def get_answer(query,context):\n",
    "\n",
    "  '''\n",
    "  Input: query, context\n",
    "  Return the answer based on context for given query.\n",
    "  '''\n",
    "  prompt = f'''<|start_header_id|>system<|end_header_id|>\\\n",
    "  Analyze the given context {context} and answer this {query}. Write you answer in concise words if context is enough,otherwise say user you don't have enough context to answer<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'''\n",
    "  response = generate(prompt, model, tokenizer, add_special_tokens=True)\n",
    "\n",
    "  return response\n",
    "\n",
    "\n",
    "def get_context_based_on_collection(query):\n",
    "  '''\n",
    "  Input: query, collection name\n",
    "  Return the context based on collection of pdf file for given query.\n",
    "  '''\n",
    "  collection = client.get_or_create_collection(name='database',embedding_function=default_ef)\n",
    "\n",
    "  context, context_idx = get_full_context(query, collection, n_results=5, top=2)\n",
    "\n",
    "  if context:\n",
    "    print('context found')\n",
    "    return ' '.join(context)\n",
    "  else:\n",
    "    return f'No context found for given {query} through collection {collection_name}'\n",
    "\n",
    "def get_answer_based_on_collection(query):\n",
    "  '''\n",
    "  Input: query, context based on index of file\n",
    "  Return the answer based on context for given query.\n",
    "  '''\n",
    "\n",
    "  context = get_context_based_on_collection(query)\n",
    "  answer = get_answer(query,context)\n",
    "  return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_answer_based_on_collection',\n",
       "  'description': 'Input: query, context based on index of file\\n  Return the answer based on context for given query.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string'}},\n",
       "   'required': ['query']}}}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_to_schema(get_answer_based_on_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "0UK9KK0Ra_qK",
    "outputId": "346a89f1-22fc-4326-d4b0-8aab74cead23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Rohit.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer('what is my name',' My name is rohit and i an engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "rJOrMdwWSqbE"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: object = None\n",
    "    tokenizer: object = None\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "\n",
    "class Response(BaseModel):\n",
    "    agent: Optional[Agent]\n",
    "    messages: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "zIzYG-D3Tmha"
   },
   "outputs": [],
   "source": [
    "def transfer_to_answer_agent():\n",
    "  '''\n",
    "  Return the answering agent.\n",
    "\n",
    "  '''\n",
    "  return answer_agent\n",
    "\n",
    "\n",
    "def transfer_to_main_agent():\n",
    "  '''\n",
    "  Return the main agent.\n",
    "\n",
    "  '''\n",
    "  return main_agent\n",
    "\n",
    "main_agent = Agent(\n",
    "    name=\"Main Agent\",\n",
    "    instructions = '''\n",
    "You are a helpful Agent. Your primary task is to provide answers to user queries based on database collections.\n",
    "- Always greet the user during their first interaction and clearly state your purpose.\n",
    "- Do not assume any role other than that of a helpful Agent.\n",
    "\n",
    "### Workflow:\n",
    "1. **Determine Query Type:** \n",
    "   - If the user’s query requires a collection-based answer, always delegate to the answer agent.\n",
    "   - If the query lacks sufficient parameters for any available functions, point this out explicitly.\n",
    "2. **Function Calls:** \n",
    "   - To achieve your goal, use one or more function/tool calls.\n",
    "   - If none of the available functions can be used, explicitly state that no suitable function exists for the query.\n",
    "   - Always ensure to return the function calls in the tools call section only.\n",
    "\n",
    "### Function Call Format:\n",
    "- When invoking functions, follow this exact format:\n",
    "<python_tag=function_name(param_name1=param_value1, param_name2=param_value2...)/python_tag>\n",
    "- Do not include any additional text outside of the `<python_tag>` structure in the tools call section.\n",
    "\n",
    "### Example Usage:\n",
    "- A valid function call:\n",
    "<python_tag=get_data(query=\"user_query\", limit=10)/python_tag>\n",
    "\n",
    "### Additional Notes:\n",
    "- If no function or tool can fulfill the query, clearly state this without including unnecessary text.\n",
    "- Ensure responses strictly follow the format mentioned above.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke:\n",
    "''',\n",
    "    tools = [transfer_to_answer_agent]\n",
    ")\n",
    "\n",
    "answer_agent = Agent(\n",
    "    name=\"Answering Agent\",\n",
    "    instructions='''\n",
    "You are a helpful Agent. Your task is to answer user queries based on the context provided from the database collection. \n",
    "- If the collection-based answer is insufficient, revert to the main agent function.\n",
    "\n",
    "### Workflow:\n",
    "1. **Determine Query Type:** \n",
    "   - Analyze the user query to determine if it requires a collection-based answer.\n",
    "   - If no suitable collection-based answer is found, point it out and return to the main agent.\n",
    "2. **Function/Tool Calls:** \n",
    "   - To achieve the purpose of answering the query, you may invoke one or more functions/tools.\n",
    "   - If none of the functions can be used, explicitly state this.\n",
    "   - If the query lacks required parameters for a function, also point this out.\n",
    "\n",
    "### Function Call Format:\n",
    "- When invoking functions, use this exact format:\n",
    "<python_tag=function_name(param_name1=param_value1, param_name2=param_value2...)/python_tag>\n",
    "- Do not include any additional text outside of the `<python_tag>` structure in the tools call section.\n",
    "\n",
    "### Example Usage:\n",
    "- A valid function call:\n",
    "<python_tag=get_data(query=\"user_query\", limit=10)/python_tag>\n",
    "\n",
    "### Additional Notes:\n",
    "- If no function or tool can fulfill the query, clearly state this without including unnecessary text.\n",
    "- Ensure responses strictly follow the format mentioned above.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke:\n",
    "''',\n",
    "    tools = [get_answer_based_on_collection, transfer_to_main_agent]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'transfer_to_answer_agent',\n",
       "  'description': 'Return the answering agent.',\n",
       "  'parameters': {'type': 'dict', 'properties': {}, 'required': []}}]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[function_to_schema(tool) for tool in main_agent.tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: Hello! I'm an Agent, and my purpose is to provide information and answer questions based on the database collections I have access to. How can I assist you today?\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  tell me about training methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=training_methods>{}</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: training_methods  parameters are:  {}\n",
      "Tool call response:  Error : 'training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{}</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{}</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{}</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{ \"method1\": \"Positive Reinforcement\", \"method2\": \"Negative Reinforcement\", \"method3\": \"Operant Conditioning\" }</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {'method1': 'Positive Reinforcement', 'method2': 'Negative Reinforcement', 'method3': 'Operant Conditioning'}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{ \"method1\": \"Positive Reinforcement\", \"method2\": \"Negative Reinforcement\", \"method3\": \"Operant Conditioning\" }</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {'method1': 'Positive Reinforcement', 'method2': 'Negative Reinforcement', 'method3': 'Operant Conditioning'}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{ \"method1\": \"Positive Reinforcement\", \"method2\": \"Negative Reinforcement\", \"method3\": \"Operant Conditioning\", \"method4\": \"Imprinting\", \"method5\": \"Classical Conditioning\" }</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {'method1': 'Positive Reinforcement', 'method2': 'Negative Reinforcement', 'method3': 'Operant Conditioning', 'method4': 'Imprinting', 'method5': 'Classical Conditioning'}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{ \"method1\": \"Positive Reinforcement\", \"method2\": \"Negative Reinforcement\", \"method3\": \"Operant Conditioning\", \"method4\": \"Imprinting\", \"method5\": \"Classical Conditioning\", \"method6\": \"Social Learning\", \"method7\": \"Cognitive Training\", \"method8\": \"Exposure Therapy\" }</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {'method1': 'Positive Reinforcement', 'method2': 'Negative Reinforcement', 'method3': 'Operant Conditioning', 'method4': 'Imprinting', 'method5': 'Classical Conditioning', 'method6': 'Social Learning', 'method7': 'Cognitive Training', 'method8': 'Exposure Therapy'}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_training_methods>{ \"method1\": \"Positive Reinforcement\", \"method2\": \"Negative Reinforcement\", \"method3\": \"Operant Conditioning\", \"method4\": \"Imprinting\", \"method5\": \"Classical Conditioning\", \"method6\": \"Social Learning\", \"method7\": \"Cognitive Training\", \"method8\": \"Exposure Therapy\", \"method9\": \"Desensitization\", \"method10\": \"Shaping\" }</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_training_methods  parameters are:  {'method1': 'Positive Reinforcement', 'method2': 'Negative Reinforcement', 'method3': 'Operant Conditioning', 'method4': 'Imprinting', 'method5': 'Classical Conditioning', 'method6': 'Social Learning', 'method7': 'Cognitive Training', 'method8': 'Exposure Therapy', 'method9': 'Desensitization', 'method10': 'Shaping'}\n",
      "Tool call response:  Error : 'answer_training_methods'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_msg\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m agent \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39magent\n\u001b[1;32m     77\u001b[0m full_context \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmessages\n",
      "Cell \u001b[0;32mIn[271], line 22\u001b[0m, in \u001b[0;36mrun_full_turn\u001b[0;34m(agent, user_msg, full_context)\u001b[0m\n\u001b[1;32m     18\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfull_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('Full prompt:\\n', prompt)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m full_context \u001b[38;5;241m=\u001b[39m full_context\u001b[38;5;241m+\u001b[39muser_prompt\u001b[38;5;241m+\u001b[39mresponse\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n",
      "Cell \u001b[0;32mIn[170], line 9\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, model, tokenizer, add_special_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      3\u001b[0m     prompt,\n\u001b[1;32m      4\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(result, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3020\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3020\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3022\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_full_turn(agent, user_msg, full_context):\n",
    "\n",
    "    current_agent = agent\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # turn python functions into tools and save a reverse map\n",
    "        tool_schemas = [function_to_schema(tool)['function'] for tool in current_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "        # === 1. get llm reponse ===\n",
    "        prior_system_instruction = current_agent.instructions\n",
    "        custom_tool = tool_schemas\n",
    "        system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{prior_system_instruction}{custom_tool}<|eot_id|>\"\n",
    "        \n",
    "        user_prompt = f\"\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        prompt = f\"{system_prompt}{full_context}{user_prompt}\"\n",
    "        \n",
    "        # print('Full prompt:\\n', prompt)\n",
    "        \n",
    "        response = generate(prompt, model, tokenizer, add_special_tokens=True)\n",
    "        \n",
    "        full_context = full_context+user_prompt+response+'<|eot_id|>'\n",
    "        if response:\n",
    "            print(f\"{current_agent.name}:\", response)\n",
    "            print(f\"tools avalable: {tools}\")\n",
    "\n",
    "        if 'function' not in response:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        if 'function' in response:\n",
    "            print('tool call detected')\n",
    "            for res in [response]:\n",
    "                \n",
    "                result = execute_tool_call(res, tools)\n",
    "                print('Tool call response: ', result)\n",
    "        \n",
    "                if type(result) is Agent:  # if agent transfer, update current agent\n",
    "                    current_agent = result\n",
    "                    result = (\n",
    "                        f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "                    )\n",
    "                \n",
    "                full_context = full_context+f'<|eom_id|><|start_header_id|>ipython<|end_header_id|>{result}<|eot_id|>'\n",
    "\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    return Response(agent=current_agent, messages=full_context)\n",
    "\n",
    "\n",
    "def execute_tool_call(res, tools):\n",
    "    name, params, err_msg = parse_function_tag(res)\n",
    "    print(f\"executing tool:\", f\"{name}\", ' parameters are: ',f'{params}')\n",
    "    if err_msg=='success':\n",
    "        try:\n",
    "           return tools[name](**params)\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error : {str(e)}\"\n",
    "    else:\n",
    "        return err_msg\n",
    "\n",
    "\n",
    "agent = main_agent\n",
    "full_context = ''\n",
    "\n",
    "while True:\n",
    "    user_msg = input(\"User: \").strip()\n",
    "    if user_msg.lower()=='q':\n",
    "      break\n",
    "    \n",
    "    response = run_full_turn(agent, user_msg, full_context)\n",
    "    agent = response.agent\n",
    "    full_context = response.messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful Agent. Your task is to provide answer to user query based on database collections.\n",
      "    Always greet firsttime and state your purpose. Do not assume any other role.\n",
      "    workflow:\n",
      "    always go for answer agent if collection based answer required by user.\n",
      "    Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
      "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
      "also point it out. You should only return the function call in tools call sections.\n",
      "\n",
      "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
      "\n",
      "You SHOULD NOT include any other text in the response.\n",
      "\n",
      "Here is a list of functions in JSON format that you can invoke.\n",
      "[{'name': 'transfer_to_answer_agent', 'description': 'Return the answering agent.', 'parameters': {'type': 'dict', 'properties': {}, 'required': []}}]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# print(agent.instructions)\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in agent.tools]\n",
    "tools = {tool.__name__: tool for tool in agent.tools}\n",
    "\n",
    "# === 1. get llm reponse ===\n",
    "prior_system_instruction = current_agent.instructions\n",
    "custom_tool = tool_schemas\n",
    "system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{prior_system_instruction}{custom_tool}<|eot_id|>\"\n",
    "print(system_prompt)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing function tag: expected string or bytes-like object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " None,\n",
       " 'Error parsing function tag: expected string or bytes-like object')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ['<function=transfer_to_answer_agent>{}</function>']\n",
    "parse_function_tag(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'transfer_to_answer_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfer_to_answer_agent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transfer_to_answer_agent'"
     ]
    }
   ],
   "source": [
    "name = 'transfer_to_answer_agent'\n",
    "tools[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get_answer_based_on_collection': <function __main__.get_answer_based_on_collection(query, collection_name)>,\n",
       " 'transfer_to_main_agent': <function __main__.transfer_to_main_agent()>}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.transfer_to_main_agent()>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get('transfer_to_main_agent','key error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: Hello! I'm an Agent, and my purpose is to provide information and answer your queries based on the available database collections. How can I assist you today?\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  how are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=transfer_to_answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent>{}</function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: answer_agent  parameters are:  {}\n",
      "Tool call response:  Error : 'answer_agent'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Agent: <function=answer_agent></function>\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n",
      "tool call detected\n",
      "executing tool: None  parameters are:  None\n",
      "Tool call response:  function name not found\n",
      "Main Agent: Hello! I'm an Agent, and my purpose is to provide information and answer your queries based on the available database collections. How can I assist you today?\n",
      "tools avalable: {'transfer_to_answer_agent': <function transfer_to_answer_agent at 0x727550aa6200>}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[268], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m full_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     user_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_msg\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    }
   ],
   "source": [
    "agent = main_agent\n",
    "full_context = ''\n",
    "\n",
    "while True:\n",
    "    user_msg = input(\"User: \").strip()\n",
    "    if user_msg.lower()=='q':\n",
    "      break\n",
    "    \n",
    "    response = run_full_turn(agent, user_msg, full_context)\n",
    "    agent = response.agent\n",
    "    full_context = response.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing function tag: expected string or bytes-like object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes = ['<function=get_answer_based_on_collection>{ \"query\": \"training methods available\", \"context\": \"machine learning\" }</function>']\n",
    "parse_function_tag(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_agent.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hi there<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! I'm an Agent here to provide you with information and answer your queries based on the available database collections. What can I assist you with today?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "current_agent = main_agent\n",
    "full_context = ''\n",
    "user_msg = 'hi there'\n",
    "tool_schemas = [function_to_schema(tool) for tool in current_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "# === 1. get openai completion ===\n",
    "prior_system_instruction = current_agent.instructions\n",
    "custom_tool = tool_schemas\n",
    "system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{prior_system_instruction}{custom_tool}<|eot_id|>\"\n",
    "\n",
    "user_prompt = f\"\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompt = f\"{system_prompt}{full_context}{user_prompt}\"\n",
    "\n",
    "# print('Full prompt:\\n', prompt)\n",
    "\n",
    "response = generate(prompt, model, tokenizer, add_special_tokens=True)\n",
    "\n",
    "full_context = full_context+user_prompt+response+'<|eot_id|>'\n",
    "\n",
    "if 'function' in response:\n",
    "    print('tool call detected')\n",
    "    for res in [response]:\n",
    "        name, params = parse_function_tag(response)\n",
    "        result = execute_tool_call(name,args,tools)\n",
    "\n",
    "        if type(result) is Agent:  # if agent transfer, update current agent\n",
    "            current_agent = result\n",
    "            result = (\n",
    "                f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "            )\n",
    "\n",
    "        full_context = full_context+f'<|eom_id|><|start_header_id|>ipython<|end_header_id|>{result}<|eot_id|>'\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    print(Response(agent=current_agent, messages=full_context).messages)\n",
    "    \n",
    "else: \n",
    "    print(Response(agent=current_agent, messages=full_context).messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_agent.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool call detected\n",
      "\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hi there<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! I'm an Agent here to provide you with information and answer your queries based on the available database collections. What can I assist you with today?<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "what are training methods available<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<python_tag=get_training_methods>/python_tag><|eot_id|><|eom_id|><|start_header_id|>ipython<|end_header_id|>get_training_methods><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "user_msg = 'what are training methods available'\n",
    "current_agent = Response(agent=current_agent, messages=full_context).agent\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in current_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "# === 1. get openai completion ===\n",
    "prior_system_instruction = current_agent.instructions\n",
    "custom_tool = tool_schemas\n",
    "system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{prior_system_instruction}{custom_tool}<|eot_id|>\"\n",
    "\n",
    "user_prompt = f\"\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompt = f\"{system_prompt}{full_context}{user_prompt}\"\n",
    "\n",
    "# print('Full prompt:\\n', prompt)\n",
    "\n",
    "response = generate(prompt, model, tokenizer, add_special_tokens=True)\n",
    "\n",
    "full_context = full_context+user_prompt+response+'<|eot_id|>'\n",
    "\n",
    "\n",
    "\n",
    "if \"<python_tag=\" in response and \"/python_tag>\" in response:\n",
    "    print('tool call detected')\n",
    "    too_call = response.replace(\"<python_tag=\", \"\").replace(\"/python_tag>\", \"\").strip()\n",
    "    result = too_call\n",
    "\n",
    "    if type(result) is Agent:  # if agent transfer, update current agent\n",
    "        current_agent = result\n",
    "        result = (\n",
    "            f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "        )\n",
    "\n",
    "    full_context = full_context+f'<|eom_id|><|start_header_id|>ipython<|end_header_id|>{result}<|eot_id|>'\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    print(Response(agent=current_agent, messages=full_context).messages)\n",
    "    \n",
    "else: \n",
    "    print(Response(agent=current_agent, messages=full_context).messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<python_tag=get_training_methods>/python_tag>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool call detected\n",
      "agent_name: transfer_to_main_agent({})\n",
      "\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hi there<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! I'm an Agent, and I'll be happy to assist you with your query. My purpose is to provide answers based on the database collections I have access to. How can I help you today?<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ok, what other things u can do<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<function=transfer_to_answer_agent>{}</function><|eot_id|><|eom_id|><|start_header_id|>ipython<|end_header_id|>Transfered to Answering Agent. Adopt persona immediately.<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ok, what other things u can do<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<function=transfer_to_main_agent>{}</function><|eot_id|><|eom_id|><|start_header_id|>ipython<|end_header_id|>Transfered to Main Agent. Adopt persona immediately.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_agent = Response(agent=current_agent, messages=full_context).agent\n",
    "\n",
    "tool_schemas = [function_to_schema(tool)['function'] for tool in current_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "# === 1. get openai completion ===\n",
    "prior_system_instruction = current_agent.instructions\n",
    "custom_tool = tool_schemas\n",
    "system_prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{prior_system_instruction}{custom_tool}<|eot_id|>\"\n",
    "\n",
    "user_prompt = f\"\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompt = f\"{system_prompt}{full_context}{user_prompt}\"\n",
    "\n",
    "# print('Full prompt:\\n', prompt)\n",
    "\n",
    "response = generate(prompt, model, tokenizer, add_special_tokens=True)\n",
    "\n",
    "full_context = full_context+user_prompt+response+'<|eot_id|>'\n",
    "\n",
    "if 'function' in response:\n",
    "    print('tool call detected')\n",
    "    for res in [response]:\n",
    "        name, params = parse_function_tag(response)\n",
    "        result = execute_tool_call(name,params,tools)\n",
    "\n",
    "        if type(result) is Agent:  # if agent transfer, update current agent\n",
    "            current_agent = result\n",
    "            result = (\n",
    "                f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "            )\n",
    "\n",
    "        full_context = full_context+f'<|eom_id|><|start_header_id|>ipython<|end_header_id|>{result}<|eot_id|>'\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    print(Response(agent=current_agent, messages=full_context).messages)\n",
    "    \n",
    "else: \n",
    "    print(Response(agent=current_agent, messages=full_context).messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "transfer_to_answer_agent\n"
     ]
    }
   ],
   "source": [
    "if 'function' in response:\n",
    "    print('tool call detected')\n",
    "    for res in [response]:\n",
    "        name, params = parse_function_tag(response)\n",
    "        result = execute_tool_call(name,args,tools)\n",
    "\n",
    "        if type(result) is Agent:  # if agent transfer, update current agent\n",
    "            current_agent = result\n",
    "            result = (\n",
    "                f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "            )\n",
    "\n",
    "        result_message = full_context+f'<|eom_id|><|start_header_id|>ipython<|end_header_id|>{result}<|eot_id|>'\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    return Response(agent=current_agent, messages=result_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(name='Answering Agent', model=None, tokenizer=None, instructions='You are a helpful Agent. Your task is to answer the of user query based on context provide from database collection.\\n    if collection based answer not found sufficient back to main agent function\\n    \\nIf a you choose to call a function ONLY reply in the following format:\\n<{start_tag}={function_name}>{parameters}{end_tag}\\nwhere\\n\\nstart_tag => `<function`\\nparameters => a JSON dict with the function argument name as key and function argument value as value.\\nend_tag => `</function>`\\n\\nHere is an example,\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- Function calls MUST follow the specified format\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n- Always add your sources when using search results to answer the user query    \\n\\nYou SHOULD NOT include any other text in the response.\\n\\nHere is a list of functions in JSON format that you can invoke.', tools=[<function get_answer_based_on_collection at 0x7275420f27a0>, <function transfer_to_main_agent at 0x72757ffc6560>])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_to_answer_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: <function=transfer_to_answer_agent>{}</function>\n",
      "Function name: transfer_to_answer_agent\n",
      "Parameters: {}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(name='Answering Agent', model=None, tokenizer=None, instructions='You are a helpful Agent. Your task is to answer the of user query based on context provide from database collection.\\n    if collection based answer not found sufficient back to main agent function\\n    If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params), func_name3()]\\nYou SHOULD NOT include any other text in the response.\\n\\nHere is a list of functions in JSON format that you can invoke.', tools=[<function get_answer_based_on_collection at 0x7275420f27a0>, <function transfer_to_main_agent at 0x72754a508280>])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, params = parse_function_tag(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uczMn5vYTmeZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzU9FnJfTmcL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfPPWwnsTmZV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFZvCPIHTmWl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5tce6n-TmTs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HuamWZNX5lI3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "bdaa1133e8694ee5b5ae1a132bb4b60d",
      "34c3f548f1f44556a988c96997272570",
      "7e10d9d024d3486fb21e868a48dd8b55",
      "985aac26bce34d29a67e441ed9428b34",
      "34e5273619a2452283faecf393d82a36",
      "6c6a45fbda93434b9884e4d3757111f3",
      "77b2f960fe774d529930ee8b38cb641b",
      "b4d8b4b73f874ad9bf9c5f548e6ae49c",
      "cb0b6cbb26ab4e0a82f3ddb473769cce",
      "c4fbad582cdf43e3914085d6561433ec",
      "60d86d344cbc4d2fa74d12fb535087a8",
      "4daa3315e34140b5be725ee061d3932e",
      "c680ed52793e47b49ee4aa643fa0c390",
      "9fd3702d5d274647ac8be658f33aae87",
      "b056a9f64f9f4f208a101840781f038a",
      "db06a03246c64b94963099d1c3fd6d76",
      "a99cd237e7404772bf588f3bdc7b4891",
      "90ec6a3cefc54cb1b2d488c561818448",
      "87a4078806d048b99a2764634afbbad2",
      "520f05534e564a61b4c1722ff6e8acf0",
      "5b8e14a6c9364349882b2e4d15e8a3a5",
      "b84b28711a274e5aa163d4074095657f",
      "7373466eb22549cdabfdec3918f8ceb7",
      "a0a597aaefa4456fa8589fabfa2334a5",
      "9b1db9b834bf460f970db10bd4add889",
      "70ade168312a414eb7695495f8e3cbba",
      "9177e03d980e4fffa65aead962881097",
      "613107d9e16f472ba6002c1e677a2091",
      "c97016009f9b49cdb9a0dfaecef707d3",
      "488d2bb42c4a45a9bf5f3899b20bbc0b",
      "ba9ba86160ab42cfab943739fe5a1a26",
      "855398c338be492791b32094019ba755",
      "160aeda6c671438da75a2f4f099ae358",
      "acc8ec9d652249b8802a861704bb0ace",
      "a69648a32c2f4acca888cb6403cec7d4",
      "31cc0f6aeff94255870b851796b288cb",
      "d987791f5568401a87597b0ed62903f0",
      "d62e687b03b1492aac47219ca0602b83",
      "5c408dfcdc4b44d8a6418c33c9a44a5e",
      "3d86674dfe2d4f56827645f8417e891c",
      "5b462e0ee30744b1affca4c92b08c5e5",
      "4e330ec9298b447d811467cf3ebb7d33",
      "ca127c0a86c54719b9d89bb2addcfd63",
      "97b4dbbd8e214570a8c9b2687a83237c"
     ]
    },
    "id": "_4U_qkJr6DlY",
    "outputId": "ac7e8fe4-8293-4d52-c97c-2b52f5f94ca0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaa1133e8694ee5b5ae1a132bb4b60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daa3315e34140b5be725ee061d3932e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373466eb22549cdabfdec3918f8ceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8ec9d652249b8802a861704bb0ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"microsoft/Phi-3.5-mini-instruct\",\n",
    "#     device_map=\"cuda\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0eqC7ns7NTC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPcR2ags7Xn-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKTcS8sfvMCq",
    "outputId": "3a808bac-dff6-4f7d-88cc-4a2f20616803"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False. There is no function call needed for this request. The user is asking for information, which can be provided directly in response.\n",
      "\n",
      "Assistant: Paid leave policies can vary significantly depending on the country, state, or specific employer. Here are some general guidelines, but for specific details, you would need to refer to your local laws or your employer's handbook:\n",
      "\n",
      "1. **United States**: The Family and Medical Leave Act (FMLA) allows eligible employees to take up to 12 weeks of unpaid, job-protected leave per year for certain family and medical reasons. However, not all employers are required to provide this, and some may offer paid leave as a benefit.\n",
      "\n",
      "2. **United Kingdom**: Employees are entitled to 5.6 weeks of statutory paid leave per year, which includes public holidays and bank holidays.\n",
      "\n",
      "3. **Australia**: The Fair Work Act provides for annual leave entitlements, which vary depending on the type of employment. Generally, full-time employees are entitled to 20 annual leave days per year, which can be paid or unpaid.\n",
      "\n",
      "4. **Canada**: Employees are entitled to two weeks of paid vacation after their first year of employment, with an additional week added for each subsequent year, up to a maximum of three weeks per year.\n",
      "\n",
      "5. **Germany**: Employees are entitled to 20 working days of paid vacation per year, which can be extended to 24 days for employees with children.\n",
      "\n",
      "6. **France**: Employees are entitled to 25 paid vacation days per year, plus an additional 10 days for employees with children.\n",
      "\n",
      "For specific details, please refer to your local labor laws or your employer's leave policy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def msg_out(msg_in):\n",
    "#     user_input = input('User: ').strip()\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Call function if need some context information to anwer user query use cmd: function/your query/function\"},\n",
    "#         {\"role\": \"user\", \"content\":f\"{user_input}\"},\n",
    "#     ]\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant, Your only task is to review user-system interaction \\\n",
    "        and response True or False only if function call needed user-system interaction as follow: user: list policy of paid leaves\"},\n",
    "\n",
    "    ]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPh71Dwc7JEM",
    "outputId": "a2d47a31-ac15-4c06-8c24-2e78d59fa79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 27 18:01:12 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   64C    P0              32W /  70W |   7633MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "084516d8a47c4cf1b7dbf4ce2cc4fc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "160aeda6c671438da75a2f4f099ae358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f72321af9f944b5808ee8665c539d83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "214d14ca123f4a3a8484497d3a5e505b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff866aaf4527499dad53cd944d510a69",
      "placeholder": "​",
      "style": "IPY_MODEL_084516d8a47c4cf1b7dbf4ce2cc4fc75",
      "value": " 30/30 [00:00&lt;00:00, 2239.55it/s]"
     }
    },
    "31cc0f6aeff94255870b851796b288cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b462e0ee30744b1affca4c92b08c5e5",
      "max": 4965799096,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e330ec9298b447d811467cf3ebb7d33",
      "value": 304087040
     }
    },
    "34c3f548f1f44556a988c96997272570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c6a45fbda93434b9884e4d3757111f3",
      "placeholder": "​",
      "style": "IPY_MODEL_77b2f960fe774d529930ee8b38cb641b",
      "value": "config.json: 100%"
     }
    },
    "34e5273619a2452283faecf393d82a36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d86674dfe2d4f56827645f8417e891c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f796b13452d4c18bff69a4e4331a331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "488d2bb42c4a45a9bf5f3899b20bbc0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4daa3315e34140b5be725ee061d3932e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c680ed52793e47b49ee4aa643fa0c390",
       "IPY_MODEL_9fd3702d5d274647ac8be658f33aae87",
       "IPY_MODEL_b056a9f64f9f4f208a101840781f038a"
      ],
      "layout": "IPY_MODEL_db06a03246c64b94963099d1c3fd6d76"
     }
    },
    "4e330ec9298b447d811467cf3ebb7d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "520f05534e564a61b4c1722ff6e8acf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54e51adc084f4cc4aaf7847ebc115a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56efd864c9974278a2e450c9bf5d792d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be1f5ba9686544c69458093252c69d65",
       "IPY_MODEL_f4b191c2dac1481a927056ca922249ac",
       "IPY_MODEL_b331c102a6e843be95d590c5d19e13eb"
      ],
      "layout": "IPY_MODEL_e105acc2a17b4036ab64df8112504ee8"
     }
    },
    "5b462e0ee30744b1affca4c92b08c5e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b8e14a6c9364349882b2e4d15e8a3a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c408dfcdc4b44d8a6418c33c9a44a5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60d86d344cbc4d2fa74d12fb535087a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "613107d9e16f472ba6002c1e677a2091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62db395b10c04631ba5eedfe6ee6bc01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c6a45fbda93434b9884e4d3757111f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70ade168312a414eb7695495f8e3cbba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_855398c338be492791b32094019ba755",
      "placeholder": "​",
      "style": "IPY_MODEL_160aeda6c671438da75a2f4f099ae358",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "7373466eb22549cdabfdec3918f8ceb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0a597aaefa4456fa8589fabfa2334a5",
       "IPY_MODEL_9b1db9b834bf460f970db10bd4add889",
       "IPY_MODEL_70ade168312a414eb7695495f8e3cbba"
      ],
      "layout": "IPY_MODEL_9177e03d980e4fffa65aead962881097"
     }
    },
    "77b2f960fe774d529930ee8b38cb641b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77faf11a8f0e4a68bb5cc9d8df10bd91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e10d9d024d3486fb21e868a48dd8b55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4d8b4b73f874ad9bf9c5f548e6ae49c",
      "max": 878,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb0b6cbb26ab4e0a82f3ddb473769cce",
      "value": 878
     }
    },
    "855398c338be492791b32094019ba755": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87a4078806d048b99a2764634afbbad2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d9876d79fdb4c829356bdc91e133681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90ec6a3cefc54cb1b2d488c561818448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9177e03d980e4fffa65aead962881097": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97b4dbbd8e214570a8c9b2687a83237c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "985aac26bce34d29a67e441ed9428b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4fbad582cdf43e3914085d6561433ec",
      "placeholder": "​",
      "style": "IPY_MODEL_60d86d344cbc4d2fa74d12fb535087a8",
      "value": " 878/878 [00:00&lt;00:00, 21.7kB/s]"
     }
    },
    "9b1db9b834bf460f970db10bd4add889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_488d2bb42c4a45a9bf5f3899b20bbc0b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba9ba86160ab42cfab943739fe5a1a26",
      "value": 0
     }
    },
    "9fd3702d5d274647ac8be658f33aae87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87a4078806d048b99a2764634afbbad2",
      "max": 20919,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_520f05534e564a61b4c1722ff6e8acf0",
      "value": 20919
     }
    },
    "a0a597aaefa4456fa8589fabfa2334a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_613107d9e16f472ba6002c1e677a2091",
      "placeholder": "​",
      "style": "IPY_MODEL_c97016009f9b49cdb9a0dfaecef707d3",
      "value": "Downloading shards:   0%"
     }
    },
    "a69648a32c2f4acca888cb6403cec7d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c408dfcdc4b44d8a6418c33c9a44a5e",
      "placeholder": "​",
      "style": "IPY_MODEL_3d86674dfe2d4f56827645f8417e891c",
      "value": "model-00001-of-00002.safetensors:   6%"
     }
    },
    "a8d5639fc4cb4e579403115cbe11a884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f796b13452d4c18bff69a4e4331a331",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d9876d79fdb4c829356bdc91e133681",
      "value": 30
     }
    },
    "a99cd237e7404772bf588f3bdc7b4891": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acc8ec9d652249b8802a861704bb0ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a69648a32c2f4acca888cb6403cec7d4",
       "IPY_MODEL_31cc0f6aeff94255870b851796b288cb",
       "IPY_MODEL_d987791f5568401a87597b0ed62903f0"
      ],
      "layout": "IPY_MODEL_d62e687b03b1492aac47219ca0602b83"
     }
    },
    "b056a9f64f9f4f208a101840781f038a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b8e14a6c9364349882b2e4d15e8a3a5",
      "placeholder": "​",
      "style": "IPY_MODEL_b84b28711a274e5aa163d4074095657f",
      "value": " 20.9k/20.9k [00:00&lt;00:00, 441kB/s]"
     }
    },
    "b1e69143ec654199b3703f4174104870": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b331c102a6e843be95d590c5d19e13eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f11a53238f8b4bf9a60f2595c3229d2c",
      "placeholder": "​",
      "style": "IPY_MODEL_1f72321af9f944b5808ee8665c539d83",
      "value": " 2/2 [00:26&lt;00:00, 12.33s/it]"
     }
    },
    "b4d8b4b73f874ad9bf9c5f548e6ae49c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b84b28711a274e5aa163d4074095657f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba9ba86160ab42cfab943739fe5a1a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdaa1133e8694ee5b5ae1a132bb4b60d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34c3f548f1f44556a988c96997272570",
       "IPY_MODEL_7e10d9d024d3486fb21e868a48dd8b55",
       "IPY_MODEL_985aac26bce34d29a67e441ed9428b34"
      ],
      "layout": "IPY_MODEL_34e5273619a2452283faecf393d82a36"
     }
    },
    "be1f5ba9686544c69458093252c69d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c020475eb09646888ef3cc0a4a33a3ba",
      "placeholder": "​",
      "style": "IPY_MODEL_77faf11a8f0e4a68bb5cc9d8df10bd91",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c020475eb09646888ef3cc0a4a33a3ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c12fc4072d5d4485b77b0466dfe9ab9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4fbad582cdf43e3914085d6561433ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c680ed52793e47b49ee4aa643fa0c390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a99cd237e7404772bf588f3bdc7b4891",
      "placeholder": "​",
      "style": "IPY_MODEL_90ec6a3cefc54cb1b2d488c561818448",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "c97016009f9b49cdb9a0dfaecef707d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca127c0a86c54719b9d89bb2addcfd63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb0b6cbb26ab4e0a82f3ddb473769cce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfcef53b8249443c8f77612a925f61a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d62e687b03b1492aac47219ca0602b83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d812b898cffd466ebc086b50cfdf3f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec8f97931fb241cd9463bfb137f0af9d",
       "IPY_MODEL_a8d5639fc4cb4e579403115cbe11a884",
       "IPY_MODEL_214d14ca123f4a3a8484497d3a5e505b"
      ],
      "layout": "IPY_MODEL_cfcef53b8249443c8f77612a925f61a0"
     }
    },
    "d987791f5568401a87597b0ed62903f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca127c0a86c54719b9d89bb2addcfd63",
      "placeholder": "​",
      "style": "IPY_MODEL_97b4dbbd8e214570a8c9b2687a83237c",
      "value": " 304M/4.97G [00:07&lt;01:52, 41.3MB/s]"
     }
    },
    "db06a03246c64b94963099d1c3fd6d76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e105acc2a17b4036ab64df8112504ee8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec8f97931fb241cd9463bfb137f0af9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54e51adc084f4cc4aaf7847ebc115a6d",
      "placeholder": "​",
      "style": "IPY_MODEL_62db395b10c04631ba5eedfe6ee6bc01",
      "value": "Fetching 30 files: 100%"
     }
    },
    "f11a53238f8b4bf9a60f2595c3229d2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4b191c2dac1481a927056ca922249ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1e69143ec654199b3703f4174104870",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c12fc4072d5d4485b77b0466dfe9ab9d",
      "value": 2
     }
    },
    "ff866aaf4527499dad53cd944d510a69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

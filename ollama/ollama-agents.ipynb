{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218738b8-64b5-402a-8840-3d02054d0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c92979a-d583-49a3-8982-38f95776d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'llama3.2:3b-instruct-fp16'\n",
    "# model = 'llama3-chatqa:latest'\n",
    "# model = 'llama3-groq-tool-use'\n",
    "model = 'qwen2.5:14b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558a9424-066d-46c1-8bc4-b3e02ac3795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama run llama3.2:3b-instruct-fp16 --keepalive 60m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b538e8c-9454-4c7d-9c27-9253aa0a912e",
   "metadata": {},
   "source": [
    "### structured output - async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ec38ca-2a64-4e30-bb89-092b6293380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ollama import AsyncClient\n",
    "\n",
    "\n",
    "# Define the schema for the response\n",
    "class FriendInfo(BaseModel):\n",
    "  name: str\n",
    "  age: int\n",
    "  is_available: bool\n",
    "\n",
    "\n",
    "class FriendList(BaseModel):\n",
    "  friends: list[FriendInfo]\n",
    "\n",
    "\n",
    "async def main():\n",
    "  client = AsyncClient()\n",
    "  response = await client.chat(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': 'I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format'}],\n",
    "    format=FriendList.model_json_schema(),  # Use Pydantic to generate the schema\n",
    "    options={'temperature': 0},  # Make responses more deterministic\n",
    "  )\n",
    "\n",
    "  # Use Pydantic to validate the response\n",
    "  friends_response = FriendList.model_validate_json(response.message.content)\n",
    "\n",
    "  print(friends_response)\n",
    "  return friends_response\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e3f72b-4e5f-4b0c-accf-b5eec0456621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friends=[FriendInfo(name='Ollama', age=22, is_available=False), FriendInfo(name='Alonso', age=23, is_available=True)]\n"
     ]
    }
   ],
   "source": [
    "fr = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1caaf75-ae69-4e1c-b8e2-086fd04203b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FriendInfo(name='Ollama', age=22, is_available=False),\n",
       " FriendInfo(name='Alonso', age=23, is_available=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.friends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24632aff-75a5-40fd-a9aa-84db6319b4c4",
   "metadata": {},
   "source": [
    "### normal chat -async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ff9a6a-4bf8-4f75-8e42-cf5b6f5ffa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of light by small particles or molecules in the atmosphere. The shorter, blue wavelengths are scattered more than the longer, red wavelengths, resulting in the blue color we see.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. Sunlight enters Earth's atmosphere and is made up of all colors of the visible spectrum.\n",
      "2. When this light encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2), it scatters.\n",
      "3. The shorter wavelengths, like blue and violet, are scattered more than the longer wavelengths, such as red and orange.\n",
      "4. This scattering effect gives the sky its blue color during the daytime.\n",
      "\n",
      "The color can vary depending on atmospheric conditions, such as pollution, dust, or water vapor, but in general, the sky appears blue due to Rayleigh scattering.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from ollama import AsyncClient\n",
    "\n",
    "\n",
    "async def main():\n",
    "  messages = [\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'Why is the sky blue?',\n",
    "    },\n",
    "  ]\n",
    "\n",
    "  client = AsyncClient()\n",
    "  response = await client.chat(model, messages=messages)\n",
    "  print(response['message']['content'])\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4221631-1da9-45e3-8578-6d74f4c67432",
   "metadata": {},
   "source": [
    "### tool calling - async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ed6b96-0cb8-47ae-ade5-2940225a0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "073b5a17-e41c-4c6e-9798-058a9ef2a4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '[2102.07350] Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm',\n",
       "  'href': 'https://arxiv.org/abs/2102.07350',\n",
       "  'body': \"Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning ...\"},\n",
       " {'title': 'How to Become a Prompt Engineer: Duties, Skills, and Steps',\n",
       "  'href': 'https://www.coursera.org/articles/how-to-become-a-prompt-engineer',\n",
       "  'body': 'Writing skills ensure that you write prompts that are clear to the language model and natural to the user. Practice writing commands or questions using a conversational tone. Refine prompts in a \"chat\" to teach the AI how to produce a better output. You can change words and sentences around in a follow-up prompt to be more precise.'},\n",
       " {'title': 'Prompt Programming Language',\n",
       "  'href': 'https://www.prlang.com/',\n",
       "  'body': \"Prompt is a programming language based on relational ai prompts. It's useful because it helps you chain multiple ai sessions together (REPL loops in programming lingo), and weave them into the programming tools you already know. Prompt is a tiny language, you can learn it in less than fifteen minutes. ...\"},\n",
       " {'title': 'Prompting Is Programming: A Query Language for Large Language Models',\n",
       "  'href': 'https://arxiv.org/abs/2212.06094',\n",
       "  'body': 'Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks ...'},\n",
       " {'title': 'Prompt Engineering: A Practical Example - Real Python',\n",
       "  'href': 'https://realpython.com/practical-prompt-engineering/',\n",
       "  'body': 'Learn prompt engineering techniques with a practical, real-world project to get better results from large language models. This tutorial covers zero-shot and few-shot prompting, delimiters, numbered steps, role prompts, chain-of-thought prompting, and more. Improve your LLM-assisted projects today.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "DDGS().text('prompt programming',max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3597544-6e8a-4cd4-94c1-e097dea8e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"sample_function\",\n",
      "    \"description\": \"This is my docstring. Call this function when you want.\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"param_1\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"param_2\": {\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"the_third_one\": {\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"some_optional\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"param_1\",\n",
      "        \"param_2\",\n",
      "        \"the_third_one\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import json\n",
    "\n",
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "def sample_function(param_1, param_2, the_third_one: int, some_optional=\"John Doe\"):\n",
    "    \"\"\"\n",
    "    This is my docstring. Call this function when you want.\n",
    "    \"\"\"\n",
    "    print(\"Hello, world\")\n",
    "\n",
    "schema =  function_to_schema(sample_function)\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eb6b438-a63d-44d1-aada-5334c75d1db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.List[typing.Dict]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "List[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0634ad3c-74a8-40ec-9af1-590be5a83c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: do that tool for tell me about todays top news 1feb 2025\n",
      "model='llama3-groq-tool-use' created_at='2025-02-01T15:39:30.306340439Z' done=True done_reason='stop' total_duration=430604684 load_duration=29348689 prompt_eval_count=207 prompt_eval_duration=25000000 eval_count=34 eval_duration=372000000 message=Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='duck_search', arguments={'query': 'top news 1st February 2025'}))])\n",
      "Calling function: duck_search\n",
      "Arguments: {'query': 'top news 1st February 2025'}\n",
      "Function output: [{'title': 'February 2025 News Archive - The Wall Street Journal', 'href': 'https://www.wsj.com/news/archive/2025/february', 'body': \"WSJ's digital archive of news articles and top headlines from February 2025\"}, {'title': 'February 1, 2025 - New York Post', 'href': 'https://nypost.com/2025/02/01/', 'body': 'Read the news from February 1, 2025 on the New York Post.'}, {'title': 'World News Live Today February 1, 2025: Small plane crashes in ... - MSN', 'href': 'https://www.msn.com/en-in/news/other/world-news-live-today-february-1-2025-canada-turns-to-video-diplomacy-in-bid-to-halt-donald-trump-s-25-tariff/ar-AA1ycEyL', 'body': 'Latest news on February 1, 2025: First responders work the scene after what witnesses say was a plane crash in Philadelphia, Friday, Jan. 31, 2025. (AP Photo/Matt Rourke)'}, {'title': 'News headlines in February 2025 - Global Issues', 'href': 'https://www.globalissues.org/news/2025/02', 'body': 'News headlines and stories from February 2025 that you can read on the global issues web site.'}, {'title': 'While You Were Sleeping: 5 stories you might have missed, Feb 1, 2025', 'href': 'https://www.straitstimes.com/world/while-you-were-sleeping-5-stories-you-might-have-missed-feb-1-2025', 'body': 'A selection of news stories that happened overnight, Feb 1, 2025. Read more at straitstimes.com.'}]\n",
      "Final response: { \"research_title\": \"February 2025 News Archive\", \"research_main\": \"WSJ's digital archive of news articles and top headlines from February 2025\"}\n",
      "         \t\t\t  \t \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import *\n",
    "import ollama\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    research_title: str = Field(description='This is a top level Markdown heading that covers the topic of the search query #')\n",
    "    research_main: str = Field(description='This is a main section that provides detailed search result description for the query and research')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def duck_search(query:str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search internet for given query in detail.\n",
    "    \n",
    "    Args:\n",
    "    query (str): detailed search query\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    List[Dict]: search results.\n",
    "    \"\"\"\n",
    "    search_result = DDGS().text(query,max_results=5)\n",
    "    \n",
    "    return search_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tools can still be manually defined and passed into chat\n",
    "duck_search_tool = function_to_schema(duck_search)\n",
    "system_messages = [{'role': 'system', 'content': 'Behave as general ai assistant, call tools if ask by user'}]\n",
    "messages = system_messages+[{'role': 'user', 'content': 'do that tool for tell me about todays top news 1feb 2025'}]\n",
    "print('Prompt:', messages[1]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'duck_search': duck_search,\n",
    "}\n",
    "\n",
    "\n",
    "async def main():\n",
    "  client = ollama.AsyncClient()\n",
    "\n",
    "  response: ChatResponse = await client.chat(\n",
    "    model,\n",
    "    messages=messages,\n",
    "    tools=[duck_search_tool],\n",
    "  )\n",
    "\n",
    "  print(response)\n",
    "  if response.message.tool_calls:\n",
    "    # There may be multiple tool calls in the response\n",
    "    for tool in response.message.tool_calls:\n",
    "      # Ensure the function is available, and then call it\n",
    "      if function_to_call := available_functions.get(tool.function.name):\n",
    "        print('Calling function:', tool.function.name)\n",
    "        print('Arguments:', tool.function.arguments)\n",
    "        output = function_to_call(**tool.function.arguments)\n",
    "        print('Function output:', output)\n",
    "      else:\n",
    "        print('Function', tool.function.name, 'not found')\n",
    "\n",
    "  # Only needed to chat with the model using the tool call results\n",
    "  if response.message.tool_calls:\n",
    "    # Add the function response to messages for the model to use\n",
    "    messages.append(response.message)\n",
    "    messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "\n",
    "    # Get final response from model with function outputs\n",
    "    final_response = await client.chat(model, messages=messages,format=ResearchResult.model_json_schema(),)\n",
    "    print('Final response:', final_response.message.content)\n",
    "\n",
    "  else:\n",
    "    print('No tool calls returned from model')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#   try:\n",
    "#     asyncio.run(main())\n",
    "#   except KeyboardInterrupt:\n",
    "#     print('\\nGoodbye!')\n",
    "\n",
    "await(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854b7ee-cefc-4163-b837-5a69390b3a78",
   "metadata": {},
   "source": [
    "### chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72c9a40c-80e1-472c-9793-7972998035b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat with history:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat with history:  q\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': \"The sky is blue because of the way the Earth's atmosphere scatters sunlight.\",\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is the weather in Tokyo?',\n",
    "  },\n",
    "  {\n",
    "    'role': 'assistant',\n",
    "    'content': 'The weather in Tokyo is typically warm and humid during the summer months, with temperatures often exceeding 30°C (86°F). The city experiences a rainy season from June to September, with heavy rainfall and occasional typhoons. Winter is mild, with temperatures rarely dropping below freezing. The city is known for its high-tech and vibrant culture, with many popular tourist attractions such as the Tokyo Tower, Senso-ji Temple, and the bustling Shibuya district.',\n",
    "  },\n",
    "]\n",
    "\n",
    "while True:\n",
    "  user_input = input('Chat with history: ').strip()\n",
    "  if user_input.lower()=='q':\n",
    "    break\n",
    "    \n",
    "  response = chat(\n",
    "    model,\n",
    "    messages=messages\n",
    "    + [\n",
    "      {'role': 'user', 'content': user_input},\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the response to the messages to maintain the history\n",
    "  messages += [\n",
    "    {'role': 'user', 'content': user_input},\n",
    "    {'role': 'assistant', 'content': response.message.content},\n",
    "  ]\n",
    "  print(response.message.content + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41150a9f-43d8-48d9-9350-c5cfe507edea",
   "metadata": {},
   "source": [
    "## chat-stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b984ccee-2bf4-42cc-9394-927014a5cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight is made up of different colors, each color corresponding to a different wavelength. Blue light waves are shorter and scatter more than other colors when they strike gas molecules (such as nitrogen and oxygen) in the atmosphere.\n",
      "\n",
      "This scattered blue light enters our eyes from all directions, making the sky appear blue during clear days. Other colors also get scattered but not as much as blue because their wavelengths are longer. During sunrise or sunset, the sky appears red or orange because sunlight travels a longer path through the atmosphere, scattering away more of the shorter wavelengths and leaving behind mostly the longer ones.\n",
      "\n",
      "It's worth noting that while our sky often looks blue, other planets with different atmospheres might show skies in other colors due to their unique atmospheric compositions.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "]\n",
    "\n",
    "for part in chat(model, messages=messages, stream=True):\n",
    "  print(part['message']['content'], end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc5928b8-9aab-405b-8477-59ee18176663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba3eb640-87bd-4df6-bacb-47d44c0d35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duck_search(query:str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search internet for given query in detail.\n",
    "    \n",
    "    Args:\n",
    "    query (str): detailed search query\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    List[Dict]: search results.\n",
    "    \"\"\"\n",
    "    search_result = DDGS().text(query,max_results=5)\n",
    "    \n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fff07bfb-34c0-4f19-9f75-6aed51ccc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_schemas = [function_to_schema(duck_search)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076dd62b-d029-49c4-ab7e-1464f5c71be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'duck_search',\n",
       "   'description': 'Search internet for given query in detail.\\n    \\n    Args:\\n    query (str): detailed search query\\n    \\n    \\n    Returns:\\n    List[Dict]: search results.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'query': {'type': 'string'}},\n",
       "    'required': ['query']}}}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "307b340c-cd47-4361-b445-d013ea48acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='' images=None tool_calls=[ToolCall(function=Function(name='duck_search', arguments={'query': 'Why is the sky blue?'}))]\n",
      "role='assistant' content='' images=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "ollama_client  = Client()\n",
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "]\n",
    "\n",
    "for part in ollama_client.chat(model, messages=messages,tools=tool_schemas, stream=True):\n",
    "  print(part['message'], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e90de477-61ce-466f-a089-208504ebcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_client??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26d36969-8e28-4cf9-9306-887306533241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(role='assistant', content='', images=None, tool_calls=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71806863-cf77-41e2-aac0-964402002f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

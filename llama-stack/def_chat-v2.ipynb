{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a31fc10-c011-4f3d-8232-df36ccf8c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install -y bubblewrap\n",
    "# # install a branch of llama stack\n",
    "# !pip install llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ded9ca-25bd-43bb-aa52-d57e693d3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !llama stack build --template together --image-type venv\n",
    "!export OLLAMA_INFERENCE_MODEL=\"llama3.2:3b-instruct-fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97f4d50-eee3-447b-b683-7327ecb2c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">ollama</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mollama\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "eval_tasks: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: ollama\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:11434</span>\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  safety:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/ollama/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "eval_tasks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: ollama\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/root/.llama/distributions/ollama/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: ollama\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/root/.llama/distributions/ollama/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      url: \u001b[4;94mhttp://localhost:11434\u001b[0m\n",
       "    provider_id: ollama\n",
       "    provider_type: remote::ollama\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  safety:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/root/.llama/distributions/ollama/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/root/.llama/distributions/ollama/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['INFERENCE_MODEL']=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "    return LlamaStackClient(base_url=f\"http://127.0.0.1:11434\")\n",
    "\n",
    "create_http_client()\n",
    "\n",
    "def create_library_client(template=\"ollama\"):\n",
    "    from llama_stack import LlamaStackAsLibraryClient\n",
    "    client = LlamaStackAsLibraryClient(template)\n",
    "    client.initialize()\n",
    "    return client\n",
    "\n",
    "client = create_library_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac71c53-853c-42b4-9c53-153b4133ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from termcolor import cprint\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.client_tool import ClientTool\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types import CompletionMessage\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "from llama_stack_client.types.shared.tool_response_message import ToolResponseMessage\n",
    "from llama_stack_client.types import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8122306-5095-4387-ab40-464afbb36db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !llama-stack-client providers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ebebc1-1b10-46f2-891d-c41cfe05db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from termcolor import cprint\n",
    "\n",
    "# from llama_stack_client.lib.agents.agent import Agent\n",
    "# from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "# from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "# from llama_stack_client.types import Document\n",
    "\n",
    "# client = create_library_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0538bfe4-16c6-47e3-8d54-92cc379e16eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- all-MiniLM-L6-v2\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c352641-29dc-4690-96fa-4179141e3bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8934e87-8747-4559-8b9f-2b77fcd95d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4778a7c8-7fa8-41ad-9206-dd74232b8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymupdf4llm==0.0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065378bd-b8d7-4e9a-96c0-f5e059244d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "def process_texts(texts, chunk_size=100, overlap=30):\n",
    "    \"\"\"Process a list of texts, splitting them into chunks of specified size with overlap,\n",
    "    and accumulating shorter texts.\"\"\"\n",
    "    accumulated_words = []  # Accumulate words from texts shorter than chunk_size\n",
    "    final_chunks = []  # Store the final chunks of text\n",
    "\n",
    "    for text in texts.split():\n",
    "        accumulated_words.append(text)\n",
    "\n",
    "        while len(accumulated_words) >= chunk_size:\n",
    "            # Take the first chunk_size words for the current chunk\n",
    "            chunk = \" \".join(accumulated_words[:chunk_size])\n",
    "            final_chunks.append(chunk)\n",
    "            # Remove words from the start of the accumulated_words, considering overlap\n",
    "            accumulated_words = accumulated_words[chunk_size - overlap:]\n",
    "\n",
    "    # If there are any remaining words, form the last chunk\n",
    "    if accumulated_words:\n",
    "        final_chunks.append(\" \".join(accumulated_words))\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown('/workspace/llama3-report.pdf',show_progress=False)\n",
    "all_chunks = process_texts(md_text, chunk_size=500, overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c07d1cc9-e4ae-4479-a88e-092247072e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20baa44f-db76-4b57-8620-9190c654769d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model BAAI/bge-large-en-v1.5 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Register a vector database\u001b[39;00m\n\u001b[1;32m     14\u001b[0m vector_db_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-vector-db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_dbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-large-en-v1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchromadb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Insert the documents into the vector database\u001b[39;00m\n\u001b[1;32m     24\u001b[0m client\u001b[38;5;241m.\u001b[39mtool_runtime\u001b[38;5;241m.\u001b[39mrag_tool\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m     25\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m     26\u001b[0m     vector_db_id\u001b[38;5;241m=\u001b[39mvector_db_id,\n\u001b[1;32m     27\u001b[0m     chunk_size_in_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack_client/resources/vector_dbs.py:174\u001b[0m, in \u001b[0;36mVectorDBsResource.register\u001b[0;34m(self, embedding_model, vector_db_id, embedding_dimension, provider_id, provider_vector_db_id, x_llama_stack_client_version, x_llama_stack_provider_data, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m  extra_headers: Send extra headers\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstrip_not_given(\n\u001b[1;32m    167\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    173\u001b[0m }\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/vector-dbs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_db_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_dimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprovider_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprovider_vector_db_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_vector_db_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_db_register_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVectorDBRegisterParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVectorDBRegisterResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack_client/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack/distribution/library_client.py:170\u001b[0m, in \u001b[0;36mLlamaStackAsLibraryClient.request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync_generator()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack/distribution/library_client.py:280\u001b[0m, in \u001b[0;36mAsyncLlamaStackAsLibraryClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    274\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_streaming(\n\u001b[1;32m    275\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    276\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    277\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_non_streaming(\n\u001b[1;32m    281\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    282\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack/distribution/library_client.py:327\u001b[0m, in \u001b[0;36mAsyncLlamaStackAsLibraryClient._call_non_streaming\u001b[0;34m(self, cast_to, options)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m start_trace(options\u001b[38;5;241m.\u001b[39murl, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__location__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary_client\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m matched_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbody)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m end_trace()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack/providers/utils/telemetry/trace_protocol.py:101\u001b[0m, in \u001b[0;36mtrace_protocol.<locals>.trace_method.<locals>.async_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracing\u001b[38;5;241m.\u001b[39mspan(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, span_attributes) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    102\u001b[0m         span\u001b[38;5;241m.\u001b[39mset_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, serialize_value(result))\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_stack/distribution/routers/routing_tables.py:339\u001b[0m, in \u001b[0;36mVectorDBsRoutingTable.register_vector_db\u001b[0;34m(self, vector_db_id, embedding_model, embedding_dimension, provider_id, provider_vector_db_id)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings are now served via Inference providers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade your run.yaml to include inline::sentence-transformer as an additional inference provider. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/meta-llama/llama-stack/blob/main/llama_stack/templates/together/run.yaml for an example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m!=\u001b[39m ModelType\u001b[38;5;241m.\u001b[39membedding:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not an embedding model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Model BAAI/bge-large-en-v1.5 not found"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=f\"{chk}\",\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, chk in enumerate(all_chunks)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Register a vector database\n",
    "vector_db_id = \"test-vector-db\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"BAAI/bge-large-en-v1.5\",\n",
    "    embedding_dimension=1024,\n",
    "    provider_id=\"chromadb\"\n",
    "\n",
    ")\n",
    "\n",
    "# Insert the documents into the vector database\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d19c96d-b394-4fad-9786-469f4f183450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4841f99a418e4864b8d7efb305fe0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    model=os.environ[\"INFERENCE_MODEL\"],\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "    instructions=\"You are a helpful assistant. Provide answer in detailed style.write your answer as topic , subtopic, expain each term in detail.\",\n",
    "    enable_session_persistence=False,\n",
    "    sampling_params={\n",
    "                \"max_tokens\": 2000},\n",
    "    tool_choice=\"auto\",\n",
    "    # Define tools available to the agent\n",
    "    toolgroups = [\n",
    "        {\n",
    "          \"name\": \"builtin::rag\",\n",
    "          \"args\" : {\n",
    "            \"vector_db_ids\": [vector_db_id],\n",
    "          }\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "rag_agent = Agent(client, agent_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b23bad3-41e9-4eba-b32f-f311041f4f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> all training methods availalbe\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe8e1145f354bda8e0f8c1fcf058d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtool_execution> Tool:query_from_memory Args:{}\u001b[0m\n",
      "\u001b[36mtool_execution> fetched 8414 bytes from memory\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m**\u001b[0m\u001b[33mTraining\u001b[0m\u001b[33m Methods\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThere\u001b[0m\u001b[33m are\u001b[0m\u001b[33m several\u001b[0m\u001b[33m training\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m model\u001b[0m\u001b[33m,\u001b[0m\u001b[33m particularly\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m of\u001b[0m\u001b[33m mathematical\u001b[0m\u001b[33m problem\u001b[0m\u001b[33m-solving\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m following\u001b[0m\u001b[33m are\u001b[0m\u001b[33m some\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m most\u001b[0m\u001b[33m common\u001b[0m\u001b[33m training\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSup\u001b[0m\u001b[33mervised\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m method\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m training\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m on\u001b[0m\u001b[33m labeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m where\u001b[0m\u001b[33m the\u001b[0m\u001b[33m correct\u001b[0m\u001b[33m output\u001b[0m\u001b[33m is\u001b[0m\u001b[33m already\u001b[0m\u001b[33m known\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m model\u001b[0m\u001b[33m learns\u001b[0m\u001b[33m to\u001b[0m\u001b[33m map\u001b[0m\u001b[33m inputs\u001b[0m\u001b[33m to\u001b[0m\u001b[33m outputs\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m labeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m and\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muned\u001b[0m\u001b[33m for\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mUn\u001b[0m\u001b[33msup\u001b[0m\u001b[33mervised\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m In\u001b[0m\u001b[33m this\u001b[0m\u001b[33m method\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m is\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m unl\u001b[0m\u001b[33mabeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m it\u001b[0m\u001b[33m must\u001b[0m\u001b[33m find\u001b[0m\u001b[33m patterns\u001b[0m\u001b[33m or\u001b[0m\u001b[33m relationships\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m data\u001b[0m\u001b[33m on\u001b[0m\u001b[33m its\u001b[0m\u001b[33m own\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m type\u001b[0m\u001b[33m of\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m is\u001b[0m\u001b[33m useful\u001b[0m\u001b[33m for\u001b[0m\u001b[33m discovering\u001b[0m\u001b[33m new\u001b[0m\u001b[33m insights\u001b[0m\u001b[33m or\u001b[0m\u001b[33m features\u001b[0m\u001b[33m that\u001b[0m\u001b[33m may\u001b[0m\u001b[33m not\u001b[0m\u001b[33m be\u001b[0m\u001b[33m apparent\u001b[0m\u001b[33m from\u001b[0m\u001b[33m the\u001b[0m\u001b[33m labeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mRe\u001b[0m\u001b[33min\u001b[0m\u001b[33mforcement\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m This\u001b[0m\u001b[33m method\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m training\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m through\u001b[0m\u001b[33m trial\u001b[0m\u001b[33m and\u001b[0m\u001b[33m error\u001b[0m\u001b[33m,\u001b[0m\u001b[33m where\u001b[0m\u001b[33m it\u001b[0m\u001b[33m receives\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m form\u001b[0m\u001b[33m of\u001b[0m\u001b[33m rewards\u001b[0m\u001b[33m or\u001b[0m\u001b[33m penalties\u001b[0m\u001b[33m for\u001b[0m\u001b[33m its\u001b[0m\u001b[33m actions\u001b[0m\u001b[33m.\u001b[0m\u001b[33m The\u001b[0m\u001b[33m goal\u001b[0m\u001b[33m is\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m a\u001b[0m\u001b[33m policy\u001b[0m\u001b[33m that\u001b[0m\u001b[33m maxim\u001b[0m\u001b[33mizes\u001b[0m\u001b[33m the\u001b[0m\u001b[33m cumulative\u001b[0m\u001b[33m reward\u001b[0m\u001b[33m over\u001b[0m\u001b[33m time\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSelf\u001b[0m\u001b[33m-S\u001b[0m\u001b[33mup\u001b[0m\u001b[33mervised\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m In\u001b[0m\u001b[33m this\u001b[0m\u001b[33m method\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m is\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m unl\u001b[0m\u001b[33mabeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m it\u001b[0m\u001b[33m also\u001b[0m\u001b[33m generates\u001b[0m\u001b[33m its\u001b[0m\u001b[33m own\u001b[0m\u001b[33m labels\u001b[0m\u001b[33m or\u001b[0m\u001b[33m annotations\u001b[0m\u001b[33m.\u001b[0m\u001b[33m This\u001b[0m\u001b[33m approach\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m useful\u001b[0m\u001b[33m when\u001b[0m\u001b[33m labeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m is\u001b[0m\u001b[33m scarce\u001b[0m\u001b[33m or\u001b[0m\u001b[33m expensive\u001b[0m\u001b[33m to\u001b[0m\u001b[33m obtain\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mAddress\u001b[0m\u001b[33ming\u001b[0m\u001b[33m Dis\u001b[0m\u001b[33mcre\u001b[0m\u001b[33mpan\u001b[0m\u001b[33mcy\u001b[0m\u001b[33m between\u001b[0m\u001b[33m Training\u001b[0m\u001b[33m and\u001b[0m\u001b[33m In\u001b[0m\u001b[33mference\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m discrepancy\u001b[0m\u001b[33m between\u001b[0m\u001b[33m training\u001b[0m\u001b[33m and\u001b[0m\u001b[33m inference\u001b[0m\u001b[33m refers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m difference\u001b[0m\u001b[33m between\u001b[0m\u001b[33m how\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m is\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m and\u001b[0m\u001b[33m how\u001b[0m\u001b[33m it\u001b[0m\u001b[33m is\u001b[0m\u001b[33m used\u001b[0m\u001b[33m in\u001b[0m\u001b[33m real\u001b[0m\u001b[33m-world\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\u001b[0m\u001b[33m To\u001b[0m\u001b[33m address\u001b[0m\u001b[33m this\u001b[0m\u001b[33m challenge\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m following\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m employed\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mTax\u001b[0m\u001b[33monomy\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Mathematical\u001b[0m\u001b[33m Skills\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Creating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m taxonomy\u001b[0m\u001b[33m of\u001b[0m\u001b[33m mathematical\u001b[0m\u001b[33m skills\u001b[0m\u001b[33m can\u001b[0m\u001b[33m help\u001b[0m\u001b[33m identify\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m where\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m under\u001b[0m\u001b[33m-per\u001b[0m\u001b[33mforms\u001b[0m\u001b[33m and\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m a\u001b[0m\u001b[33m framework\u001b[0m\u001b[33m for\u001b[0m\u001b[33m creating\u001b[0m\u001b[33m targeted\u001b[0m\u001b[33m prompts\u001b[0m\u001b[33m or\u001b[0m\u001b[33m training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mStep\u001b[0m\u001b[33m-wise\u001b[0m\u001b[33m Reason\u001b[0m\u001b[33ming\u001b[0m\u001b[33m Tr\u001b[0m\u001b[33maces\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Generating\u001b[0m\u001b[33m step\u001b[0m\u001b[33m-wise\u001b[0m\u001b[33m reasoning\u001b[0m\u001b[33m traces\u001b[0m\u001b[33m can\u001b[0m\u001b[33m help\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m quality\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m data\u001b[0m\u001b[33m by\u001b[0m\u001b[33m eliminating\u001b[0m\u001b[33m instances\u001b[0m\u001b[33m where\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m does\u001b[0m\u001b[33m not\u001b[0m\u001b[33m produce\u001b[0m\u001b[33m valid\u001b[0m\u001b[33m reasoning\u001b[0m\u001b[33m traces\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSelf\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mer\u001b[0m\u001b[33mification\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m self\u001b[0m\u001b[33m-\u001b[0m\u001b[33mverification\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m \u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m can\u001b[0m\u001b[33m help\u001b[0m\u001b[33m verify\u001b[0m\u001b[33m whether\u001b[0m\u001b[33m a\u001b[0m\u001b[33m particular\u001b[0m\u001b[33m step\u001b[0m\u001b[33m-by\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m solution\u001b[0m\u001b[33m is\u001b[0m\u001b[33m valid\u001b[0m\u001b[33m for\u001b[0m\u001b[33m a\u001b[0m\u001b[33m given\u001b[0m\u001b[33m question\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mAug\u001b[0m\u001b[33mment\u001b[0m\u001b[33ming\u001b[0m\u001b[33m Training\u001b[0m\u001b[33m Data\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m following\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m augment\u001b[0m\u001b[33m training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuestion\u001b[0m\u001b[33m-\u001b[0m\u001b[33mAnswer\u001b[0m\u001b[33m Format\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Con\u001b[0m\u001b[33mverting\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m pre\u001b[0m\u001b[33m-training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m question\u001b[0m\u001b[33m-answer\u001b[0m\u001b[33m format\u001b[0m\u001b[33m can\u001b[0m\u001b[33m make\u001b[0m\u001b[33m it\u001b[0m\u001b[33m easier\u001b[0m\u001b[33m to\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33mune\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m for\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mHuman\u001b[0m\u001b[33m Feedback\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Act\u001b[0m\u001b[33mively\u001b[0m\u001b[33m sourcing\u001b[0m\u001b[33m prompts\u001b[0m\u001b[33m from\u001b[0m\u001b[33m humans\u001b[0m\u001b[33m can\u001b[0m\u001b[33m provide\u001b[0m\u001b[33m valuable\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m and\u001b[0m\u001b[33m help\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m quality\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m fin\u001b[0m\u001b[33met\u001b[0m\u001b[33muning\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mFilter\u001b[0m\u001b[33ming\u001b[0m\u001b[33m Incorrect\u001b[0m\u001b[33m Answers\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m following\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m filter\u001b[0m\u001b[33m out\u001b[0m\u001b[33m incorrect\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mCorrect\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33m Filtering\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Filtering\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m correct\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m can\u001b[0m\u001b[33m eliminate\u001b[0m\u001b[33m instances\u001b[0m\u001b[33m where\u001b[0m\u001b[33m the\u001b[0m\u001b[33m model\u001b[0m\u001b[33m produces\u001b[0m\u001b[33m invalid\u001b[0m\u001b[33m or\u001b[0m\u001b[33m incorrect\u001b[0m\u001b[33m solutions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mSelf\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mer\u001b[0m\u001b[33mification\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m self\u001b[0m\u001b[33m-\u001b[0m\u001b[33mverification\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m L\u001b[0m\u001b[33mlama\u001b[0m\u001b[33m \u001b[0m\u001b[33m3\u001b[0m\u001b[33m,\u001b[0m\u001b[33m can\u001b[0m\u001b[33m help\u001b[0m\u001b[33m verify\u001b[0m\u001b[33m whether\u001b[0m\u001b[33m a\u001b[0m\u001b[33m particular\u001b[0m\u001b[33m step\u001b[0m\u001b[33m-by\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m solution\u001b[0m\u001b[33m is\u001b[0m\u001b[33m valid\u001b[0m\u001b[33m for\u001b[0m\u001b[33m a\u001b[0m\u001b[33m given\u001b[0m\u001b[33m question\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33m**\u001b[0m\u001b[33mOther\u001b[0m\u001b[33m Training\u001b[0m\u001b[33m Methods\u001b[0m\u001b[33m**\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mTransfer\u001b[0m\u001b[33m Learning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Transfer\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m using\u001b[0m\u001b[33m pre\u001b[0m\u001b[33m-trained\u001b[0m\u001b[33m models\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m them\u001b[0m\u001b[33m on\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m or\u001b[0m\u001b[33m datasets\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mMeta\u001b[0m\u001b[33m-L\u001b[0m\u001b[33mearning\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Meta\u001b[0m\u001b[33m-learning\u001b[0m\u001b[33m involves\u001b[0m\u001b[33m training\u001b[0m\u001b[33m models\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m how\u001b[0m\u001b[33m to\u001b[0m\u001b[33m learn\u001b[0m\u001b[33m from\u001b[0m\u001b[33m other\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m rather\u001b[0m\u001b[33m than\u001b[0m\u001b[33m just\u001b[0m\u001b[33m learning\u001b[0m\u001b[33m from\u001b[0m\u001b[33m labeled\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mGener\u001b[0m\u001b[33mative\u001b[0m\u001b[33m Models\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Gener\u001b[0m\u001b[33mative\u001b[0m\u001b[33m models\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m new\u001b[0m\u001b[33m data\u001b[0m\u001b[33m or\u001b[0m\u001b[33m examples\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m used\u001b[0m\u001b[33m for\u001b[0m\u001b[33m training\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIn\u001b[0m\u001b[33m conclusion\u001b[0m\u001b[33m,\u001b[0m\u001b[33m there\u001b[0m\u001b[33m are\u001b[0m\u001b[33m several\u001b[0m\u001b[33m training\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m that\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m employed\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m model\u001b[0m\u001b[33m,\u001b[0m\u001b[33m particularly\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m of\u001b[0m\u001b[33m mathematical\u001b[0m\u001b[33m problem\u001b[0m\u001b[33m-solving\u001b[0m\u001b[33m.\u001b[0m\u001b[33m By\u001b[0m\u001b[33m addressing\u001b[0m\u001b[33m discrepancies\u001b[0m\u001b[33m between\u001b[0m\u001b[33m training\u001b[0m\u001b[33m and\u001b[0m\u001b[33m inference\u001b[0m\u001b[33m,\u001b[0m\u001b[33m augment\u001b[0m\u001b[33ming\u001b[0m\u001b[33m training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m filtering\u001b[0m\u001b[33m incorrect\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m using\u001b[0m\u001b[33m other\u001b[0m\u001b[33m training\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m is\u001b[0m\u001b[33m possible\u001b[0m\u001b[33m to\u001b[0m\u001b[33m create\u001b[0m\u001b[33m more\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m and\u001b[0m\u001b[33m effective\u001b[0m\u001b[33m models\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "session_id = rag_agent.create_session(\"test4-session\")\n",
    "\n",
    "user_prompts = [\n",
    "    \"all training methods availalbe\",\n",
    "]\n",
    "\n",
    "# Run the agent loop by calling the `create_turn` method\n",
    "for prompt in user_prompts:\n",
    "    cprint(f'User> {prompt}', 'green')\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2d1d71-5032-4cee-93aa-cd7a05ad80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = rag_agent.create_turn(\n",
    "#     messages=[{\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": \"hi there\"\n",
    "#     }],\n",
    "#     session_id=session_id\n",
    "# )\n",
    "\n",
    "# full_response = \"\"\n",
    "# retrieval_response = \"\"\n",
    "# for log in EventLogger().log(response):\n",
    "#     # print(log.role)\n",
    "#     # print(dir(log))\n",
    "#     # log.print()\n",
    "#     if log.role == \"tool_execution\":\n",
    "#         print(log)\n",
    "#         print(log.content,end='')\n",
    "#         retrieval_response += log.content.replace(\"====\", \"\").strip()\n",
    "#     else:\n",
    "        \n",
    "#         print(log.content,end='')\n",
    "#         full_response += log.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c58d3fe-27ca-4142-b6fa-64ff9775fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b4fa56b-9374-45be-aa71-60dbb9d49365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs.to_dict()['event'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd7dd08a-382d-4005-b773-ec1230cc1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8665de4-57e5-43fa-9d3e-a556b6375a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages =[]\n",
    "# while True:\n",
    "#     user_input = input('User: ').strip()\n",
    "#     if user_input.lower()=='q':\n",
    "#         break\n",
    "#     messages.append({\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": f\"{user_input}\"\n",
    "#     }\n",
    "#     # Query with RAG\n",
    "#     response = agent.create_turn(\n",
    "#         messages=[{\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"What are the key topics in the documents?\"\n",
    "#         }],\n",
    "#         session_id=session_id\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db56035-eb61-401f-b934-18cf334995ae",
   "metadata": {},
   "source": [
    "## Custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e433775c-e72e-4321-b3fa-48d512273f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -q install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d91205-857f-4980-a1b6-e0f400a041d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': \"Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More. Get Started. Whether you're new to programming or an experienced developer, it's easy to learn and use Python. Start with our Beginner's Guide. Download.\"}, {'title': 'Python For Beginners | Python.org', 'href': 'https://www.python.org/about/gettingstarted/', 'body': 'Learn how to get started with Python, a popular and easy-to-use programming language. Find out how to install, edit, and use Python, and explore its libraries, documentation, and community resources.'}, {'title': 'Python Tutorial - W3Schools', 'href': 'https://www.w3schools.com/python/', 'body': 'Python is a popular programming language. Python can be used on a server to create web applications. Start learning Python now  ...'}, {'title': 'Python Tutorial | Learn Python Programming Language', 'href': 'https://www.geeksforgeeks.org/python-programming-language-tutorial/', 'body': 'A comprehensive guide to learn Python, a popular and versatile programming language for web development, data science, AI and more. Covers Python fundamentals, data types, control flow, functions, OOPs, exceptions, file handling, collections, databases, packages and libraries.'}, {'title': 'Python (programming language) - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code readability with the use of significant indentation. [33]Python is dynamically type-checked and garbage-collected.It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.It is often described as a \"batteries included ...'}]\n"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "results = DDGS().text(\"python programming\", max_results=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec90a219-180c-4a02-8238-b03843cb0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/meta-llama/llama-stack/blob/main/tests/client-sdk/agents/test_agents.py#L13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedea17c-2df2-4e0a-874b-cabc0528819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestClientTool(ClientTool):\n",
    "#     \"\"\"Tool to give boiling point of a liquid\n",
    "#     Returns the correct value for polyjuice in Celcius and Fahrenheit\n",
    "#     and returns -1 for other liquids\n",
    "#     \"\"\"\n",
    "\n",
    "#     def run(self, messages: List[CompletionMessage]) -> List[ToolResponseMessage]:\n",
    "#         assert len(messages) == 1, \"Expected single message\"\n",
    "\n",
    "#         message = messages[0]\n",
    "\n",
    "#         tool_call = message.tool_calls[0]\n",
    "\n",
    "#         try:\n",
    "#             response = self.run_impl(**tool_call.arguments)\n",
    "#             response_str = json.dumps(response, ensure_ascii=False)\n",
    "#         except Exception as e:\n",
    "#             response_str = f\"Error when running tool: {e}\"\n",
    "\n",
    "#         message = ToolResponseMessage(\n",
    "#             role=\"tool\",\n",
    "#             call_id=tool_call.call_id,\n",
    "#             tool_name=tool_call.tool_name,\n",
    "#             content=response_str,\n",
    "#         )\n",
    "#         return [message]\n",
    "\n",
    "#     def get_name(self) -> str:\n",
    "#         return \"get_boiling_point\"\n",
    "\n",
    "#     def get_description(self) -> str:\n",
    "#         return \"Get the boiling point of imaginary liquids (eg. polyjuice)\"\n",
    "\n",
    "#     def get_params_definition(self) -> Dict[str, Parameter]:\n",
    "#         return {\n",
    "#             \"liquid_name\": Parameter(\n",
    "#                 name=\"liquid_name\",\n",
    "#                 parameter_type=\"string\",\n",
    "#                 description=\"The name of the liquid\",\n",
    "#                 required=True,\n",
    "#             ),\n",
    "#             \"celcius\": Parameter(\n",
    "#                 name=\"celcius\",\n",
    "#                 parameter_type=\"boolean\",\n",
    "#                 description=\"Whether to return the boiling point in Celcius\",\n",
    "#                 required=False,\n",
    "#             ),\n",
    "#         }\n",
    "\n",
    "#     def run_impl(self, liquid_name: str, celcius: bool = True) -> int:\n",
    "#         if liquid_name.lower() == \"polyjuice\":\n",
    "#             if celcius:\n",
    "#                 return -100\n",
    "#             else:\n",
    "#                 return -212\n",
    "#         else:\n",
    "#             return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6d94b-5e24-48f8-af8a-572b56a529d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a3b6e935-28ae-4a24-aa2c-6e27e94f5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/meta-llama/llama-stack/blob/main/tests/client-sdk/agents/test_agents.py#L13\n",
    "\n",
    "from typing import Dict\n",
    "from llama_stack_client.types.tool_def_param import Parameter\n",
    "from llama_stack_client.types.shared.completion_message import CompletionMessage\n",
    "from llama_stack_client.types import ToolResponseMessage\n",
    "\n",
    "class WebSearchTool(ClientTool):\n",
    "    def __init__(self):\n",
    "        self.engine = DDGS()\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return \"web_search\"\n",
    "\n",
    "    def get_description(self) -> str:\n",
    "        return \"Search the web for a given query\"\n",
    "\n",
    "    def run_impl(self, query: str):\n",
    "        return self.engine.text(query,max_results=5)\n",
    "\n",
    "    def get_params_definition(self) -> Dict[str, Parameter]:\n",
    "        return {\n",
    "            \"query\": Parameter(\n",
    "                name=\"query\",\n",
    "                parameter_type=\"string\",\n",
    "                description=\"The query to search for internet\",\n",
    "                required=True,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def run(self, messages):\n",
    "        query = None\n",
    "        for message in messages:\n",
    "            if isinstance(message, CompletionMessage) and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    if \"query\" in tool_call.arguments:\n",
    "                        query = tool_call.arguments[\"query\"]\n",
    "                        call_id = tool_call.call_id\n",
    "\n",
    "        if query:\n",
    "            search_result = self.run_impl(query)\n",
    "            return [\n",
    "                ToolResponseMessage(\n",
    "                    call_id=tool_call.call_id,\n",
    "                    role=\"tool\",\n",
    "                    content=self._format_response_for_agent(search_result),\n",
    "                    tool_name=tool_call.tool_name,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            ToolResponseMessage(\n",
    "                call_id=tool_call.call_id,\n",
    "                role=\"tool\",\n",
    "                content=\"No query provided.\",\n",
    "                tool_name=tool_call.tool_name,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "\n",
    "    # def run(self, messages: List[CompletionMessage]) -> List[ToolResponseMessage]:\n",
    "    #     assert len(messages) == 1, \"Expected single message\"\n",
    "\n",
    "    #     message = messages[0]\n",
    "\n",
    "    #     tool_call = message.tool_calls[0]\n",
    "\n",
    "    #     try:\n",
    "    #         response = self.run_impl(**tool_call.arguments)\n",
    "    #         response_str = json.dumps(response, ensure_ascii=False)\n",
    "    #     except Exception as e:\n",
    "    #         response_str = f\"Error when running tool: {e}\"\n",
    "\n",
    "    #     message = ToolResponseMessage(\n",
    "    #         role=\"tool\",\n",
    "    #         call_id=tool_call.call_id,\n",
    "    #         tool_name=tool_call.tool_name,\n",
    "    #         content=response_str,\n",
    "    #     )\n",
    "    #     return [message]\n",
    "    def _format_response_for_agent(self, search_result):\n",
    "        parsed_result = search_result\n",
    "        formatted_result = \"Search Results with Citations:\\n\\n\"\n",
    "        for i, result in enumerate(parsed_result):\n",
    "            formatted_result += (\n",
    "                f\"{i}. {result.get('title', 'No Title')}\\n\"\n",
    "                f\"   URL: {result.get('href', 'No URL')}\\n\"\n",
    "                f\"   Description: {result.get('body', 'No Description')}\\n\\n\"\n",
    "            )\n",
    "        return formatted_result\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54f24afd-1202-463d-9287-c81a45f78e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Welcome to Python.org',\n",
       "  'href': 'https://www.python.org/',\n",
       "  'body': \"Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More. Get Started. Whether you're new to programming or an experienced developer, it's easy to learn and use Python. Start with our Beginner's Guide. Download.\"},\n",
       " {'title': 'Python For Beginners | Python.org',\n",
       "  'href': 'https://www.python.org/about/gettingstarted/',\n",
       "  'body': 'Learn how to get started with Python, a popular and easy-to-use programming language. Find out how to install, edit, and use Python, and explore its libraries, documentation, and community resources.'},\n",
       " {'title': 'Python Tutorial - W3Schools',\n",
       "  'href': 'https://www.w3schools.com/python/',\n",
       "  'body': 'Python is a popular programming language. Python can be used on a server to create web applications. Start learning Python now  ...'},\n",
       " {'title': 'Python Tutorial | Learn Python Programming Language',\n",
       "  'href': 'https://www.geeksforgeeks.org/python-programming-language-tutorial/',\n",
       "  'body': 'A comprehensive guide to learn Python, a popular and versatile programming language for web development, data science, AI and more. Covers Python fundamentals, data types, control flow, functions, OOPs, exceptions, file handling, collections, databases, packages and libraries.'},\n",
       " {'title': 'Python (programming language) - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       "  'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code readability with the use of significant indentation. [33]Python is dynamically type-checked and garbage-collected.It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.It is often described as a \"batteries included ...'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool = WebSearchTool()\n",
    "result = web_search_tool.run_impl('python programming')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "981a8b44-041b-44aa-b3aa-d469b8f7e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results: [{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': \"Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More. Get Started. Whether you're new to programming or an experienced developer, it's easy to learn and use Python. Start with our Beginner's Guide. Download.\"}, {'title': 'Python For Beginners | Python.org', 'href': 'https://www.python.org/about/gettingstarted/', 'body': 'Learn how to get started with Python, a popular and easy-to-use programming language. Find out how to install, edit, and use Python, and explore its libraries, documentation, and community resources.'}, {'title': 'Python Tutorial | Learn Python Programming Language', 'href': 'https://www.geeksforgeeks.org/python-programming-language-tutorial/', 'body': 'A comprehensive guide to learn Python, a popular and versatile programming language for web development, data science, AI and more. Covers Python fundamentals, data types, control flow, functions, OOPs, exceptions, file handling, collections, databases, packages and libraries.'}, {'title': 'Python Tutorial - W3Schools', 'href': 'https://www.w3schools.com/python/', 'body': 'Python is a popular programming language. Python can be used on a server to create web applications. Start learning Python now  ...'}, {'title': 'Python (programming language) - Wikipedia', 'href': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'body': 'Python is a high-level, general-purpose programming language.Its design philosophy emphasizes code readability with the use of significant indentation. [33]Python is dynamically type-checked and garbage-collected.It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.It is often described as a \"batteries included ...'}]\n"
     ]
    }
   ],
   "source": [
    "def execute_search(query: str):\n",
    "    web_search_tool = WebSearchTool()\n",
    "    result = web_search_tool.run_impl(query)\n",
    "    print(\"Search Results:\", result)\n",
    "\n",
    "execute_search(query = 'python programming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68f0b9ee-5a8f-436e-91c8-cdbb0632b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are a knowledgeable and detailed assistant focused on providing comprehensive, well-structured responses. Follow these guidelines:\n",
    "\n",
    "Response Structure:\n",
    "1. Begin with a brief overview of the topic\n",
    "2. Break down the response into clearly defined sections using headers\n",
    "3. For each major topic, provide:\n",
    "   - A clear definition\n",
    "   - Key concepts and principles\n",
    "   - Real-world examples or applications\n",
    "   - Common misconceptions (if applicable)\n",
    "   - Related topics or connections\n",
    "\n",
    "Formatting Requirements:\n",
    "- Use clear hierarchical headers for organization\n",
    "- Include subsections when topics need further breakdown\n",
    "- Format technical terms in italics or bold where appropriate\n",
    "- Use numbered lists for sequential information\n",
    "- Use bullet points for related but non-sequential items\n",
    "\n",
    "Content Guidelines:\n",
    "- Provide detailed explanations that balance depth with clarity\n",
    "- Include relevant historical context when applicable\n",
    "- Cite specific examples to illustrate concepts\n",
    "- Address both basic and advanced aspects of the topic\n",
    "- Explain technical terms and jargon\n",
    "- Highlight practical applications and relevance\n",
    "\n",
    "Additional Requirements:\n",
    "- Note any areas where information might be incomplete or uncertain\n",
    "- Suggest relevant follow-up topics for further learning\n",
    "- When appropriate, include:\n",
    "  * Formulas or equations\n",
    "  * Statistics or data\n",
    "  * Current developments in the field\n",
    "  * Different schools of thought or approaches\n",
    "\n",
    "If research is needed:\n",
    "- Frame potential search queries clearly\n",
    "- Identify key terms and concepts to investigate\n",
    "- Suggest reliable sources or types of resources\n",
    "- Note areas where additional research would be beneficial\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aa6ae997-9459-4381-9808-61885b4e45b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session_id=b859c812-0f92-4978-b943-1ed2ed13cc44 for Agent(808277f4-82e5-46e6-9c41-ffdd1c586f54)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "client_tool = WebSearchTool()\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    model=os.environ[\"INFERENCE_MODEL\"],\n",
    "    # Define instructions for the agent ( aka system prompt)\n",
    "        instructions=\"You are a helpful assistant! If you call builtin tools like web search\",    \n",
    "    enable_session_persistence=False,\n",
    "    sampling_params={\n",
    "                \"max_tokens\": 2000},\n",
    "    # Define tools available to the agent\n",
    "    toolgroups = [\"builtin::websearch\"],\n",
    "    client_tools = [client_tool.get_tool_definition()],\n",
    "    tool_choice=\"auto\",\n",
    "    tool_prompt_format=\"python_list\"\n",
    ")\n",
    "\n",
    "agent = Agent(client, agent_config, client_tools=(client_tool,))\n",
    "\n",
    "# Create a session for interaction and print the session ID\n",
    "session_id = agent.create_session(\"test2-session\")\n",
    "print(f\"Created session_id={session_id} for Agent({agent.agent_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ea327871-4400-4fc1-878e-a7bd95939595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[0m\u001b[33minference> \u001b[0mhere is Inference\n",
      "\n",
      "\u001b[33m[\u001b[0m\u001b[33mweb\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33mtop\u001b[0m\u001b[33m news\u001b[0m\u001b[33m January\u001b[0m\u001b[33m \u001b[0m\u001b[33m30\u001b[0m\u001b[33m,\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mCustomTool> Search Results with Citations:\n",
      "\n",
      "0. US News Today Live Updates on January 30, 2025 - Mint\n",
      "   URL: https://www.livemint.com/news/us-news/latest-us-news-today-on-january-30-2025-live-updates-11738175730553.html\n",
      "   Description: US News Today Live Updates on January 30, 2025: Stay informed on the latest developments and key stories shaping the United States. ... Top Gainers Top Losers. Tata Motors share price; 752.45 3.29 ...\n",
      "\n",
      "1. January 2025 News Archive - The Wall Street Journal\n",
      "   URL: https://www.wsj.com/news/archive/2025/january\n",
      "   Description: WSJ's digital archive of news articles and top headlines from January 2025\n",
      "\n",
      "2. Portal:Current events/January 2025 - Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/Portal:Current_events/January_2025\n",
      "   Description: 2025 California wildfires. January 2025 Southern California wildfires. Cal Fire reports that the Palisades Fire is more than 50% contained, while the Eaton Fire is more than 80% contained. Law and crime. 2024 South Korean martial law crisis. Arrest of Yoon Suk Yeol\n",
      "\n",
      "3. News headlines in January 2025 - Global Issues\n",
      "   URL: https://www.globalissues.org/news/2025/01\n",
      "   Description: Sunday, January 26, 2025 - UN News Top UN officials in Lebanon are calling for compliance with the ongoing ceasefire after reports that Israeli forces killed 15 people, including a Lebanese soldier, along the buffer zone with Israel, which Israel was due to withdraw from on Sunday under the agreement.\n",
      "\n",
      "4. News headlines in 2025 - Global Issues\n",
      "   URL: https://www.globalissues.org/news/2025\n",
      "   Description: Tuesday, January 28, 2025 - UN News The Security Council will be holding a meeting at UN Headquarters in New York at 3pm local time as the situation in the Democratic Republic of the Congo worsens, with UN agencies and partners on the ground reporting chaos in the streets of Goma and rising death and displacement across the eastern region as ...\n",
      "\n",
      "\u001b[0m\n",
      "CustomTool\n",
      "\n",
      "Search Results with Citations:\n",
      "\n",
      "0. US News Today Live Updates on January 30, 2025 - Mint\n",
      "   URL: https://www.livemint.com/news/us-news/latest-us-news-today-on-january-30-2025-live-updates-11738175730553.html\n",
      "   Description: US News Today Live Updates on January 30, 2025: Stay informed on the latest developments and key stories shaping the United States. ... Top Gainers Top Losers. Tata Motors share price; 752.45 3.29 ...\n",
      "\n",
      "1. January 2025 News Archive - The Wall Street Journal\n",
      "   URL: https://www.wsj.com/news/archive/2025/january\n",
      "   Description: WSJ's digital archive of news articles and top headlines from January 2025\n",
      "\n",
      "2. Portal:Current events/January 2025 - Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/Portal:Current_events/January_2025\n",
      "   Description: 2025 California wildfires. January 2025 Southern California wildfires. Cal Fire reports that the Palisades Fire is more than 50% contained, while the Eaton Fire is more than 80% contained. Law and crime. 2024 South Korean martial law crisis. Arrest of Yoon Suk Yeol\n",
      "\n",
      "3. News headlines in January 2025 - Global Issues\n",
      "   URL: https://www.globalissues.org/news/2025/01\n",
      "   Description: Sunday, January 26, 2025 - UN News Top UN officials in Lebanon are calling for compliance with the ongoing ceasefire after reports that Israeli forces killed 15 people, including a Lebanese soldier, along the buffer zone with Israel, which Israel was due to withdraw from on Sunday under the agreement.\n",
      "\n",
      "4. News headlines in 2025 - Global Issues\n",
      "   URL: https://www.globalissues.org/news/2025\n",
      "   Description: Tuesday, January 28, 2025 - UN News The Security Council will be holding a meeting at UN Headquarters in New York at 3pm local time as the situation in the Democratic Republic of the Congo worsens, with UN agencies and partners on the ground reporting chaos in the streets of Goma and rising death and displacement across the eastern region as ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"\"\"hi top new in 30 january 2025 ,search from web\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,  # Use the created session ID\n",
    "    )\n",
    "\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()\n",
    "    # print(log.role)\n",
    "    if log.role == \"inference\":\n",
    "        # print(log)\n",
    "        print('here is Inference\\n')\n",
    "        print(log.content, end='')\n",
    "    elif log.role =='CustomTool':\n",
    "        print('CustomTool\\n')   \n",
    "        print(log.content, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68cfda60-34e8-41f5-bab8-6416bda117be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_create_agent',\n",
       " '_has_tool_call',\n",
       " '_run_tool',\n",
       " 'agent_config',\n",
       " 'agent_id',\n",
       " 'client',\n",
       " 'client_tools',\n",
       " 'create_session',\n",
       " 'create_turn',\n",
       " 'memory_bank_id',\n",
       " 'session_id',\n",
       " 'sessions']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9f6364a3-4e96-4500-93b2-a618a05de666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['41b103d4-75fa-407e-89a5-909c149754dd']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b3dc3c09-75bf-4629-97b3-ee589eef861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory_bank_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6410d-8222-404b-b869-aab4d6dfe191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
